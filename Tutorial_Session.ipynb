{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presence/Absence qPCR Demo Session\n",
    "\n",
    "In this demo, we will review:\n",
    "\n",
    "- Implementing fully connected neural networks using Keras\n",
    "- Changing parameters within the Keras models and observing the effect on performance\n",
    "- Achieving the performance statistics cited during this week's presentation \n",
    "\n",
    "Prior to beginning this session, it is assumed that you have already downloaded Demo_Files.zip and moved the contents into a desired folder on your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 - Setting up\n",
    "\n",
    "To begin, locate the file path to the folder on your system storing the files from Demo_Files.zip. The \"folder_path\" string variable in the cell below currently holds a dummy file path string for reference. Change it to the file path on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder_path = '/Users/christopher.chin/Documents/Project_Part_1/Demo_Files'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to load the modules, random seed initializations, functions, and data you will be using in the rest of this demo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopher.chin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/christopher.chin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/christopher.chin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Modules to Import and random seeds to set\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(1234)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "import scipy.io\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import mlab\n",
    "import pylab as pl\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import scipy\n",
    "import pandas \n",
    "import keras\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import Callback, TensorBoard\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import Input, Dense, Dropout, Activation, BatchNormalization, Flatten, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, GlobalAveragePooling1D, LSTM, Dropout, Bidirectional\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from matplotlib.pyplot import imshow\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "class data_and_labels:    \n",
    "    ''' data_and_labels Class\n",
    "            Reads Tom's Dataset and extracts the curves and corresponding annotations\n",
    "            \n",
    "            Args:\n",
    "                filenames: list of string filenames for each Excel spreadsheet in the dataset\n",
    "    '''\n",
    "\n",
    "    def __init__(self,filenames):\n",
    "        self.filenames = filenames\n",
    "        # all_X is an array of shape (64001,40), comprising the 64001 normalized curves\n",
    "        # that are 40 points each \n",
    "        self.all_X = []\n",
    "        # all_X is an array of shape (64001,40), comprising the 64001 raw, unnormalized\n",
    "        # curves that are 40 points each \n",
    "        self.all_X_raw = []\n",
    "        # all_Y is an array of shape (64001,1) comprising the annotations for the 64001\n",
    "        # curves\n",
    "        self.all_Y = []\n",
    "        # all_numcurves is an int variable to continuously keep track of how many curves\n",
    "        # have been analyzed so far\n",
    "        self.all_numcurves = []\n",
    "\n",
    "    def excel_to_curves(self,filename):\n",
    "        ''' excel_to_curves Method\n",
    "                Takes one Excel spreadsheet and extracts the curves, corresponding annotations,\n",
    "                and number of points in each curve \n",
    "            \n",
    "                Args:\n",
    "                    filename: string name of file to analyze\n",
    "                Returns:\n",
    "                    file_labels: array containing the annotations of each curve\n",
    "                    file_numcycles: array containing the number of points in each curve\n",
    "                    file_curves: array containing all the curves in the file\n",
    "        '''\n",
    "        file = pandas.read_excel(io=filename,sheet_name='Sheet1',header=0,usecols='F,K:AY')\n",
    "        file = file.values\n",
    "        file_labels = file[:,0]\n",
    "        file_numcycles = file[:,1]\n",
    "        file_curves = file[:,2:]\n",
    "        file_curves = np.transpose(file_curves)\n",
    "        return file_labels,file_numcycles,file_curves\n",
    "\n",
    "    def normalize_curves(self,file_curves):\n",
    "        ''' normalize_curves Method\n",
    "                Takes all the curves from an Excel spreadsheet and normalizes them from\n",
    "                0 to 1\n",
    "            \n",
    "                Args:\n",
    "                    file_curves: array containing all the curves in the file of interest\n",
    "                Returns:\n",
    "                    file_curves_normalized: array containing all the normalized curves\n",
    "        '''\n",
    "        min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "        file_curves_normalized = min_max_scaler.fit_transform(file_curves)\n",
    "        return file_curves_normalized\n",
    "    \n",
    "    def sample_to_features(self,sample,numcycles):\n",
    "        ''' sample_to_features Method\n",
    "                Takes one curve and converts it into features. If a curve is less than \n",
    "                40 points, pad the last value of the curve until the curve is 40 points\n",
    "                long. \n",
    "            \n",
    "                Args:\n",
    "                    sample: array containing all the points in the curve\n",
    "                    numcycles: int number of points in the curve\n",
    "                Returns:\n",
    "                    sample_y: array containing the curve padded to 40 points. \n",
    "        '''\n",
    "        sample_y = sample\n",
    "        if numcycles < 40:\n",
    "            while len(sample_y) < 40:\n",
    "                sample_y = np.append(sample_y,sample_y[len(sample_y)-1])\n",
    "        sample_y = np.reshape(sample_y,(1,len(sample_y)))\n",
    "        return sample_y\n",
    "    \n",
    "    def curves_to_fileXY(self,file_labels,file_numcycles,file_curves_normalized,file_curves):\n",
    "        ''' curves_to_fileXY Method\n",
    "                Takes all the curves in one Excel spreadsheet, converts them into features, \n",
    "                and concatenates them into one array. Does the same for all the annotations \n",
    "                in the spreadsheet.\n",
    "            \n",
    "                Args:\n",
    "                    file_labels: array containing all the annotations in the file\n",
    "                    file_numcycles: array containing the number of points per curve in the\n",
    "                                    file\n",
    "                    file_curves_normalized: array containing all the normalized curves \n",
    "                                            in the file before conversion to features\n",
    "                    file_curves: array containing all the raw, unnormalized curves in the\n",
    "                                 file before conversion to features\n",
    "                Returns:\n",
    "                    file_X: array containing all the normalized curves, now all 40 points long \n",
    "                    file_X_raw: array containing all the raw, unnormalized curves, now all \n",
    "                                40 points long\n",
    "                    file_Y: array containing all the annotations of the curves\n",
    "        '''\n",
    "        file_numcurves = np.shape(file_curves_normalized)[1]\n",
    "        self.all_numcurves = np.append(self.all_numcurves,file_numcurves)\n",
    "        file_X = []\n",
    "        file_X_raw = []\n",
    "        file_Y = []\n",
    "        for ii in range(file_numcurves):\n",
    "            sample_y = self.sample_to_features(file_curves_normalized[:,ii],file_numcycles[ii])\n",
    "            sample_y_raw = self.sample_to_features(file_curves[:,ii],file_numcycles[ii])\n",
    "            if len(file_X) == 0:\n",
    "                file_X = sample_y\n",
    "                file_X_raw = sample_y_raw \n",
    "                file_Y = file_labels[ii]\n",
    "            else:\n",
    "                file_X = np.append(file_X,sample_y,axis=0)\n",
    "                file_X_raw = np.append(file_X_raw,sample_y_raw,axis=0)\n",
    "                file_Y = np.append(file_Y,file_labels[ii])\n",
    "        return file_X,file_X_raw,file_Y\n",
    "    \n",
    "    def allexcel_to_allXY(self): \n",
    "        ''' allexcel_to_allXY Method\n",
    "                Master function that calls the other functions - takes all the spreadsheets\n",
    "                in the dataset, extracts the curves and annotations, converts the curves to\n",
    "                features, performs normalization, and concatenates all the curves into one \n",
    "                master array all_X, and all the annotations into one master array all_Y\n",
    "\n",
    "        '''\n",
    "        for ii in range(len(self.filenames)):\n",
    "            file_labels,file_numcycles,file_curves = self.excel_to_curves(filenames[ii])\n",
    "            file_curves_normalized = self.normalize_curves(file_curves)\n",
    "            file_X,file_X_raw,file_Y = self.curves_to_fileXY(file_labels,file_numcycles,file_curves_normalized,file_curves)\n",
    "            if len(self.all_X) == 0:\n",
    "                self.all_X = file_X\n",
    "                self.all_X_raw = file_X_raw\n",
    "                self.all_Y = file_Y\n",
    "            else:\n",
    "                self.all_X = np.append(self.all_X,file_X,axis=0)\n",
    "                self.all_X_raw = np.append(self.all_X_raw,file_X_raw,axis=0)\n",
    "                self.all_Y = np.append(self.all_Y,file_Y)\n",
    "                \n",
    "    def grandindex_to_filename_and_row(self,grandindex):\n",
    "        ''' grandindex_to_filename_and_row Method\n",
    "                After separating into training and test sets, the master arrays all_x\n",
    "                and all_Y are shuffled, losing the connection between the curve and the\n",
    "                file it came from. Pandas DataFrames are used to maintain the indices \n",
    "                from the original master arrays. This function converts these indices\n",
    "                into the corresponding file and the row in the file you can find\n",
    "                the curve of interest\n",
    "            \n",
    "                Args:\n",
    "                    grandindex: Pandas DataFrame index of the curve of interest\n",
    "                Returns:\n",
    "                    filename: string name of file that the curve came from\n",
    "                    row: int row number in the file that you can find the curve\n",
    "        '''\n",
    "        total = 0\n",
    "        row = 0\n",
    "        for ii in range(len(self.all_numcurves)): \n",
    "            if (grandindex > total + self.all_numcurves[ii]):\n",
    "                total = total + self.all_numcurves[ii]\n",
    "            else: \n",
    "                # 2 is added because python indexes start at 0 but curves in Tom's \n",
    "                # Excel spreadsheets start from row 2 down. \n",
    "                row = grandindex - total + 2\n",
    "                filename = self.filenames[ii]\n",
    "                break\n",
    "        return filename,row\n",
    "            \n",
    "    def find_error_indices(self,Y_pred,Y_truth):\n",
    "        ''' find_error_indices Method\n",
    "                Takes the array of predictions Y_pred and the array of ground truth \n",
    "                annotations Y_test and compares them elementwise. If they differ, \n",
    "                the index is added to the returned array error_indices\n",
    "            \n",
    "                Args:\n",
    "                    Y_pred: array of predictions from the model\n",
    "                    Y_truth: array of ground truth annotations for the predictions\n",
    "                Returns:\n",
    "                    error_indices: array of indices in the input arrays where the \n",
    "                                   values differed (indicating an error)\n",
    "        '''\n",
    "        Y_truth_temp = Y_truth.values\n",
    "        error_indices = []\n",
    "        for ii in range(len(Y_truth_temp)):\n",
    "            if Y_truth_temp[ii] != Y_pred[ii]:\n",
    "                error_indices = np.append(error_indices,ii)\n",
    "        return error_indices\n",
    "\n",
    "    def check_error_samples(self,Y_pred,Y_truth,error_indices,num_to_check):\n",
    "        ''' check_error_samples Method\n",
    "                For a specified number of misclassfied examples to find\n",
    "                (num_to_check) and the input array of all indices corresponding \n",
    "                to misclassified examples (error_indices), randomly extract \n",
    "                num_to_check misclassified examples and output the file and row\n",
    "                that that example can be found\n",
    "            \n",
    "                Args:\n",
    "                    Y_pred: array of predictions from the model\n",
    "                    Y_truth: array of ground truth annotations for the predictions\n",
    "                    error_indices: array containing the indices of all misclassified\n",
    "                                   examples\n",
    "                    num_to_check: int number of misclassified examples to find\n",
    "                Returns:\n",
    "                    error_samples: Dictionary containing num_to_check dictionaries\n",
    "                                   Each sub-dictionary contains three keys and values\n",
    "                                   corresponding to the prediction of a misclassified\n",
    "                                   example, annotation of a misclassified example, and\n",
    "                                   tuple of the filename and row that that misclassified\n",
    "                                   example can be found\n",
    "        '''\n",
    "        error_samples = dict()\n",
    "        for ii in range(num_to_check):\n",
    "            rand_array_index = np.random.randint(len(error_indices))\n",
    "            rand_error_index = int(error_indices[rand_array_index])\n",
    "            grandindex = Y_truth.iloc[rand_error_index:rand_error_index+1].index[0]\n",
    "            label = Y_truth.iloc[rand_error_index]\n",
    "            prediction = Y_pred[rand_error_index]\n",
    "            filename,row = self.grandindex_to_filename_and_row(grandindex)\n",
    "            error_samples[ii] = dict()\n",
    "            error_samples[ii]['Prediction'] = prediction\n",
    "            error_samples[ii]['Label'] = label\n",
    "            error_samples[ii]['Location'] = (filename,row)\n",
    "        return error_samples    \n",
    "                \n",
    "    def add_noise_data(self,numcurves): \n",
    "        ''' add_noise_data Method\n",
    "                Add random noise to the dataset, with annotation -1 (non-amplified)\n",
    "            \n",
    "                Args:\n",
    "                    numcurves: number of random noise curves to add to the dataset\n",
    "\n",
    "        '''\n",
    "        for ii in range(numcurves):\n",
    "            noise_data = np.random.randn(1,40)\n",
    "            noise_label = 0\n",
    "            self.all_X = np.append(self.all_X,noise_data,axis=0)\n",
    "            self.all_Y = np.append(self.all_Y,noise_label)\n",
    "\n",
    "def compute_confmat_3classes(y_pred, y_true):\n",
    "    ''' compute_confmat_3classes Method\n",
    "            Given the predictions and corresponding annotations, construct the \n",
    "            confusion matrix and output its contents \n",
    "\n",
    "            Args:\n",
    "                y_pred: array of predictions from the model\n",
    "                y_true: array of ground truth annotations for the predictions\n",
    "\n",
    "            Returns:\n",
    "                df_conf_mat: array corresponding to the confusion matrix table \n",
    "                fpr: false positive rate \n",
    "                fnr: false negative rate\n",
    "                tpr: true positive rate\n",
    "                tnr: true negative rate\n",
    "                inc_pos: Number of inc annotations called positive\n",
    "                inc_inc: Number of inc annotations called inc\n",
    "                inc_neg: Number of inc annotations called negative \n",
    "                    \n",
    "    '''\n",
    "\n",
    "    conf_mat = np.zeros((3, 3))\n",
    "    row_labels = ['Actual: -1','Actual: 0','Actual: 1']\n",
    "    col_labels = ['Predicted: -1','Predicted: 0','Predicted: 1']\n",
    "    y_true_copy = np.copy(y_true)\n",
    "    y_pred_copy = np.copy(y_pred)\n",
    "    for ii in range(K.flatten(y_true).shape[0].value):\n",
    "        conf_mat[int(y_true_copy[ii])][int(y_pred_copy[ii])] += 1\n",
    "    df_conf_mat = pandas.DataFrame(conf_mat,row_labels,col_labels)\n",
    "    fpr = df_conf_mat.values[0,2]/(df_conf_mat.values[0,0]+df_conf_mat.values[0,2])\n",
    "    fnr = df_conf_mat.values[2,0]/(df_conf_mat.values[2,0]+df_conf_mat.values[2,2])\n",
    "    tpr = df_conf_mat.values[2,2]/(df_conf_mat.values[2,0]+df_conf_mat.values[2,2])\n",
    "    tnr = df_conf_mat.values[0,0]/(df_conf_mat.values[0,0]+df_conf_mat.values[0,2])\n",
    "    inc_pos = df_conf_mat.values[1,2]\n",
    "    inc_inc = df_conf_mat.values[1,1]\n",
    "    inc_neg = df_conf_mat.values[1,0]\n",
    "    pos_inc = df_conf_mat.values[2,1]\n",
    "    neg_inc = df_conf_mat.values[0,1]\n",
    "    return df_conf_mat,fpr,fnr,tpr,tnr,inc_pos,inc_inc,inc_neg,pos_inc,neg_inc\n",
    "\n",
    "def cv_trainingset(model_name,X_train,Y_train,X_test,Y_test,numsamples,cv_dict,aux_dict=dict(),class_weight_dict=dict()): \n",
    "    ''' cv_trainingset Method\n",
    "            Performs Stratified K-Fold Cross Validation on the training set, where \"numsamples\"\n",
    "            defines the number of folds. Averages the performance metrics across all the folds \n",
    "            and returns the mean and standard deviation of those performance metrics in \"all_dict.\"\n",
    "            \"Stratified\" K-Fold refers to preserving the class distribution of the entire training \n",
    "            set in each fold. \n",
    "\n",
    "            Args:\n",
    "                model_name: string name of the model to load \n",
    "                X_train: array containing the training set (curves)\n",
    "                Y_train: array containing the training annotations \n",
    "                X_test: array containing the test set (curves)\n",
    "                Y_test: array containing the test set annotations\n",
    "                numsamples: number of folds\n",
    "                cv_dict: Dictionary containing pre-determined indices to split the training\n",
    "                         set into \"numsamples\" folds.\n",
    "                aux_dict: Dictionary containing auxiliary inputs for the training \n",
    "                          and test set (if applicable). Defaults to empty dictionary\n",
    "                class_weight_dict: Dictionary containing class weights for fitting the model\n",
    "                                   (if applicable). Defaults to empty dictionary\n",
    "\n",
    "            Returns:\n",
    "                all_dict: dictionary containing the mean and standard deviation of \n",
    "                          the performance metrics for the numsamples runs. \n",
    "                    \n",
    "    '''\n",
    "    all_acc_train = []\n",
    "    all_fpr_train = []\n",
    "    all_fnr_train = []\n",
    "    all_tpr_train = []\n",
    "    all_tnr_train = []\n",
    "    all_inc_pos_train = []\n",
    "    all_inc_inc_train = []\n",
    "    all_inc_neg_train = []\n",
    "    all_pos_inc_train = []\n",
    "    all_neg_inc_train = []\n",
    "    all_acc_val = []\n",
    "    all_fpr_val = []\n",
    "    all_fnr_val = []\n",
    "    all_tpr_val = []\n",
    "    all_tnr_val = []\n",
    "    all_inc_pos_val = []\n",
    "    all_inc_inc_val = []\n",
    "    all_inc_neg_val = []\n",
    "    all_pos_inc_val = []\n",
    "    all_neg_inc_val = []\n",
    "    all_acc_test = []\n",
    "    all_fpr_test = []\n",
    "    all_fnr_test = []\n",
    "    all_tpr_test = []\n",
    "    all_tnr_test = []\n",
    "    all_inc_pos_test = []\n",
    "    all_inc_inc_test = []\n",
    "    all_inc_neg_test = []\n",
    "    all_pos_inc_test = []\n",
    "    all_neg_inc_test = []\n",
    "    all_dict = dict()\n",
    "    # Execute numsamples times\n",
    "    for ii in range(numsamples):\n",
    "        os.environ['PYTHONHASHSEED'] = '0'\n",
    "        np.random.seed(42)\n",
    "        rn.seed(12345)\n",
    "        tf.set_random_seed(1234)\n",
    "        model = load_model(model_name)\n",
    "        sample_indices_train = cv_dict[ii]['train']\n",
    "        sample_indices_val = cv_dict[ii]['val']\n",
    "        X_train_sample = X_train[sample_indices_train]\n",
    "        Y_train_sample = Y_train[sample_indices_train]\n",
    "        X_val_sample = X_train[sample_indices_val]\n",
    "        Y_val_sample = Y_train[sample_indices_val]\n",
    "        Y_train_argmax = np.argmax(Y_train_sample,axis=1)\n",
    "        Y_val_argmax = np.argmax(Y_val_sample,axis=1)\n",
    "        # If aux_dict is not an empty dictionary, extract the auxiliary inputs \n",
    "        # corresponding to the training and test sets \n",
    "        if bool(aux_dict):\n",
    "            X_train_aux_input = aux_dict['X_train_aux_input']\n",
    "            X_train_aux_input_sample = X_train_aux_input[sample_indices_train]\n",
    "            X_val_aux_input_sample = X_train_aux_input[sample_indices_val]\n",
    "            X_test_aux_input = aux_dict['X_test_aux_input']\n",
    "            if bool(class_weight_dict):\n",
    "                model.fit([X_train_sample,X_train_aux_input_sample],Y_train_sample,epochs=10,batch_size=200,class_weight=class_weight_dict)\n",
    "            else:\n",
    "                model.fit([X_train_sample,X_train_aux_input_sample],Y_train_sample,epochs=10,batch_size=200)\n",
    "            Y_pred_train = np.squeeze(model.predict([X_train_sample,X_train_aux_input_sample]))\n",
    "            Y_pred_train = Y_pred_train.argmax(axis=-1)\n",
    "            Y_pred_val = np.squeeze(model.predict([X_val_sample,X_val_aux_input_sample]))\n",
    "            Y_pred_val = Y_pred_val.argmax(axis=-1)\n",
    "            Y_pred_test = np.squeeze(model.predict([X_test,X_test_aux_input]))\n",
    "            Y_pred_test = Y_pred_test.argmax(axis=-1)\n",
    "        else:\n",
    "            if bool(class_weight_dict):\n",
    "                model.fit(X_train_sample,Y_train_sample,epochs=10,batch_size=200,class_weight=class_weight_dict)\n",
    "            else:\n",
    "                model.fit(X_train_sample,Y_train_sample,epochs=10,batch_size=200)\n",
    "            Y_pred_train = np.squeeze(model.predict(X_train_sample))\n",
    "            Y_pred_train = Y_pred_train.argmax(axis=-1)\n",
    "            Y_pred_val = np.squeeze(model.predict(X_val_sample))\n",
    "            Y_pred_val = Y_pred_val.argmax(axis=-1)\n",
    "            Y_pred_test = np.squeeze(model.predict(X_test))\n",
    "            Y_pred_test = Y_pred_test.argmax(axis=-1)\n",
    "        # Calculate the number of errors by comparing the annotations and \n",
    "        # predictions elementwise and summing the number of times they differ\n",
    "        numerrors = (Y_train_argmax != Y_pred_train).sum()\n",
    "        _, fpr, fnr, tpr, tnr, inc_pos, inc_inc, inc_neg, pos_inc, neg_inc = compute_confmat_3classes(Y_pred_train, Y_train_argmax)\n",
    "        # Calculate accuracy \n",
    "        acc = (len(Y_pred_train)-numerrors)/len(Y_pred_train)\n",
    "        all_acc_train.append(acc)\n",
    "        all_fpr_train.append(fpr)\n",
    "        all_fnr_train.append(fnr)\n",
    "        all_tpr_train.append(tpr)\n",
    "        all_tnr_train.append(tnr)\n",
    "        all_inc_pos_train.append(inc_pos)\n",
    "        all_inc_inc_train.append(inc_inc)\n",
    "        all_inc_neg_train.append(inc_neg)\n",
    "        all_pos_inc_train.append(pos_inc)\n",
    "        all_neg_inc_train.append(neg_inc)\n",
    "        # Repeat the procedure for the validation set \n",
    "        numerrors = (Y_val_argmax != Y_pred_val).sum()\n",
    "        _, fpr, fnr, tpr, tnr, inc_pos, inc_inc, inc_neg, pos_inc, neg_inc = compute_confmat_3classes(Y_pred_val, Y_val_argmax)\n",
    "        acc = (len(Y_pred_val)-numerrors)/len(Y_pred_val)\n",
    "        all_acc_val.append(acc)\n",
    "        all_fpr_val.append(fpr)\n",
    "        all_fnr_val.append(fnr)\n",
    "        all_tpr_val.append(tpr)\n",
    "        all_tnr_val.append(tnr)\n",
    "        all_inc_pos_val.append(inc_pos)\n",
    "        all_inc_inc_val.append(inc_inc)\n",
    "        all_inc_neg_val.append(inc_neg)\n",
    "        all_pos_inc_val.append(pos_inc)\n",
    "        all_neg_inc_val.append(neg_inc)\n",
    "        # Repeat the procedure for the test set \n",
    "        numerrors = (Y_test != Y_pred_test).sum()\n",
    "        _, fpr, fnr, tpr, tnr, inc_pos, inc_inc, inc_neg, pos_inc, neg_inc = compute_confmat_3classes(Y_pred_test, Y_test)\n",
    "        acc = (len(Y_pred_test)-numerrors)/len(Y_pred_test)\n",
    "        all_acc_test.append(acc)\n",
    "        all_fpr_test.append(fpr)\n",
    "        all_fnr_test.append(fnr)\n",
    "        all_tpr_test.append(tpr)\n",
    "        all_tnr_test.append(tnr)\n",
    "        all_inc_pos_test.append(inc_pos)\n",
    "        all_inc_inc_test.append(inc_inc)\n",
    "        all_inc_neg_test.append(inc_neg)\n",
    "        all_pos_inc_test.append(pos_inc)\n",
    "        all_neg_inc_test.append(neg_inc)\n",
    "    # Performance metrics for the training set:\n",
    "    # Accuracy\n",
    "    all_dict['acc_train'] = [np.mean(all_acc_train),np.std(all_acc_train)]\n",
    "    # False Positive Rate\n",
    "    all_dict['fpr_train'] = [np.mean(all_fpr_train),np.std(all_fpr_train)]\n",
    "    # False Negative Rate\n",
    "    all_dict['fnr_train'] = [np.mean(all_fnr_train),np.std(all_fnr_train)]\n",
    "    # True Positive Rate\n",
    "    all_dict['tpr_train'] = [np.mean(all_tpr_train),np.std(all_tpr_train)]\n",
    "    # True Negative Rate\n",
    "    all_dict['tnr_train'] = [np.mean(all_tnr_train),np.std(all_tnr_train)]\n",
    "    # Number of inc annotations called positive\n",
    "    all_dict['inc_pos_train'] = [np.mean(all_inc_pos_train),np.std(all_inc_pos_train)]\n",
    "    # Number of inc annotations called inc\n",
    "    all_dict['inc_inc_train'] = [np.mean(all_inc_inc_train),np.std(all_inc_inc_train)]\n",
    "    # Number of inc annotations called negative\n",
    "    all_dict['inc_neg_train'] = [np.mean(all_inc_neg_train),np.std(all_inc_neg_train)]\n",
    "    # Number of positive annotations called inc\n",
    "    all_dict['pos_inc_train'] = [np.mean(all_pos_inc_train),np.std(all_pos_inc_train)]\n",
    "    # Number of negative annotations called inc\n",
    "    all_dict['neg_inc_train'] = [np.mean(all_neg_inc_train),np.std(all_pos_inc_train)]\n",
    "    # Performance metrics for the validation set:\n",
    "    all_dict['acc_val'] = [np.mean(all_acc_val),np.std(all_acc_val)]\n",
    "    all_dict['fpr_val'] = [np.mean(all_fpr_val),np.std(all_fpr_val)]\n",
    "    all_dict['fnr_val'] = [np.mean(all_fnr_val),np.std(all_fnr_val)]\n",
    "    all_dict['tpr_val'] = [np.mean(all_tpr_val),np.std(all_tpr_val)]\n",
    "    all_dict['tnr_val'] = [np.mean(all_tnr_val),np.std(all_tnr_val)]\n",
    "    all_dict['inc_pos_val'] = [np.mean(all_inc_pos_val),np.std(all_inc_pos_val)]\n",
    "    all_dict['inc_inc_val'] = [np.mean(all_inc_inc_val),np.std(all_inc_inc_val)]\n",
    "    all_dict['inc_neg_val'] = [np.mean(all_inc_neg_val),np.std(all_inc_neg_val)]\n",
    "    all_dict['pos_inc_val'] = [np.mean(all_pos_inc_val),np.std(all_pos_inc_val)]\n",
    "    all_dict['neg_inc_val'] = [np.mean(all_neg_inc_val),np.std(all_pos_inc_val)]\n",
    "    # Performance metrics for the test set:\n",
    "    all_dict['acc_test'] = [np.mean(all_acc_test),np.std(all_acc_test)]\n",
    "    all_dict['fpr_test'] = [np.mean(all_fpr_test),np.std(all_fpr_test)]\n",
    "    all_dict['fnr_test'] = [np.mean(all_fnr_test),np.std(all_fnr_test)]\n",
    "    all_dict['tpr_test'] = [np.mean(all_tpr_test),np.std(all_tpr_test)]\n",
    "    all_dict['tnr_test'] = [np.mean(all_tnr_test),np.std(all_tnr_test)]\n",
    "    all_dict['inc_pos_test'] = [np.mean(all_inc_pos_test),np.std(all_inc_pos_test)]\n",
    "    all_dict['inc_inc_test'] = [np.mean(all_inc_inc_test),np.std(all_inc_inc_test)]\n",
    "    all_dict['inc_neg_test'] = [np.mean(all_inc_neg_test),np.std(all_inc_neg_test)]\n",
    "    all_dict['pos_inc_test'] = [np.mean(all_pos_inc_test),np.std(all_pos_inc_test)]\n",
    "    all_dict['neg_inc_test'] = [np.mean(all_neg_inc_test),np.std(all_pos_inc_test)]\n",
    "    return all_dict\n",
    "\n",
    "def create_training_and_test_sets(all_X,all_Y,test_size=0.2):\n",
    "    ''' create_training_and_test_sets Method\n",
    "            Shuffle the entire dataset (all_X) and set of annotations (all_Y).\n",
    "            Then split into a training set and test set, the proportions of \n",
    "            which are determined by the input \"test_size.\" In addition, \n",
    "            one-hot-encode the training set annotations for use by Keras\n",
    "\n",
    "            Args:\n",
    "                all_X: Pandas DataFrame of all the curves in the dataset\n",
    "                all_Y: Pandas DataFrame of all the annotations in the dataset\n",
    "                test_size: fraction of the entire dataset that should be set\n",
    "                           aside for the test set\n",
    "\n",
    "            Returns:\n",
    "                X_train: Pandas DataFrame containing the training set (curves)\n",
    "                Y_train: array containing the one-hot-encoded training annotations \n",
    "                X_test: Pandas DataFrame containing the test set (curves)\n",
    "                Y_test: Pandas DataFrame containing the test set annotations\n",
    "                    \n",
    "    '''\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(all_X,all_Y,test_size)\n",
    "    # Adjust annotations to comply with Keras encoding, which cannot accept \n",
    "    # label of -1. Convert -1 to 0, 0 to 1, and 1 to 2. \n",
    "    Y_train += 1\n",
    "    Y_test += 1\n",
    "    # Convert annotations to Keras one-hot-encoding\n",
    "    Y_train = keras.utils.to_categorical(Y_train,num_classes=3)\n",
    "    return X_train,X_test,Y_train,Y_test\n",
    "\n",
    "# Load in the dataset of Excel spreadsheets\n",
    "filenames = glob.glob(folder_path+'/*.xls')\n",
    "# Use the data_and_labels class to extract the curves and annotations\n",
    "# from the Excel spreadsheets \n",
    "all_XY = data_and_labels(filenames)\n",
    "all_XY.allexcel_to_allXY()\n",
    "# all_X holds the curves in the dataset and all_Y holds the annotations\n",
    "# in the dataset\n",
    "all_X = pandas.DataFrame(all_XY.all_X)\n",
    "all_Y = pandas.Series(all_XY.all_Y) \n",
    "\n",
    "# Load in the data for all auxiliary inputs\n",
    "# RMS Noise\n",
    "mat_contents = scipy.io.loadmat(folder_path+'/RMSNoise.mat')\n",
    "RMSNoise = mat_contents['approxRMSNoise']\n",
    "RMSNoise = np.swapaxes(RMSNoise,0,1)\n",
    "# Curve Derivatives\n",
    "mat_contents = scipy.io.loadmat(folder_path+'/Derivs_RMS.mat')\n",
    "Derivs_RMS = mat_contents['zerothAndHigherOrderDerivs_RMS']\n",
    "Derivs_RMS = np.swapaxes(Derivs_RMS,0,2)\n",
    "# Number of Polynomial Curve Crossings\n",
    "mat_contents = scipy.io.loadmat(folder_path+'/numPolyCrossings.mat')\n",
    "numPolyCrossings = mat_contents['numPolyCrossings']\n",
    "numPolyCrossings = np.swapaxes(numPolyCrossings,0,1)\n",
    "# Number of Diagonal Line Crossings\n",
    "mat_contents = scipy.io.loadmat(folder_path+'/numZeroCrossingsDiagonal.mat')\n",
    "numZeroCrossingsDiagonal = mat_contents['numZeroCrossingsDiagonal']\n",
    "numZeroCrossingsDiagonal = np.swapaxes(numZeroCrossingsDiagonal,0,1)\n",
    "# Number of Horizontal Line Crossings\n",
    "mat_contents = scipy.io.loadmat(folder_path+'/numZeroCrossingsHorizontal.mat')\n",
    "numZeroCrossingsHorizontal = mat_contents['numZeroCrossingsHorizontal']\n",
    "numZeroCrossingsHorizontal = np.swapaxes(numZeroCrossingsHorizontal,0,1)\n",
    "# Location of Second Derivative Maxima\n",
    "mat_contents = scipy.io.loadmat(folder_path+'/yMaxMinLocs.mat')\n",
    "yMaxMinLocs = mat_contents['yMaxMinLocs']\n",
    "yMaxMinLocs = np.swapaxes(yMaxMinLocs,0,1)\n",
    "# Value of Second Derivative Maxima\n",
    "mat_contents = scipy.io.loadmat(folder_path+'/yMaxMinVals.mat')\n",
    "yMaxMinVals = mat_contents['yMaxMinVals']\n",
    "yMaxMinVals = np.swapaxes(yMaxMinVals,0,1)\n",
    "# RMS Noise Scaling Factor\n",
    "mat_contents = scipy.io.loadmat(folder_path+'/RMSNoiseScalingFactor.mat')\n",
    "RMSNoiseScalingFactor = mat_contents['approxRMSNoiseBasedCurvesScalingFactor']\n",
    "RMSNoiseScalingFactor = np.swapaxes(RMSNoiseScalingFactor,0,1)\n",
    "# numZeroCrossingsDiagonal contains two columns, the first for positive slope diagonal line\n",
    "# crossings, and the second for negative slope diagonal line crossings.\n",
    "# More strongly weight the number of negative slope diagonal line crossings (weighting of 2)\n",
    "numZeroCrossingsDiagonal[:,1]*=2\n",
    "# Combine the two diagonal line crossing numbers into one summary number\n",
    "numZeroCrossingsDiagonal = np.sum(numZeroCrossingsDiagonal,axis=1)\n",
    "numZeroCrossingsDiagonal = numZeroCrossingsDiagonal[:,np.newaxis]\n",
    "# numZeroCrossingsHorizontal contains several columns, each for the number of crossings\n",
    "# for horizontal lines with different slopes and intercepts. Combine all the numbers into\n",
    "# one summary number.\n",
    "numZeroCrossingsHorizontal = np.sum(numZeroCrossingsHorizontal,axis=1)\n",
    "numZeroCrossingsHorizontal = numZeroCrossingsHorizontal[:,np.newaxis]\n",
    "# Combine the number of horizontal and diagonal line crossings into one summary number\n",
    "numZeroCrossings = np.concatenate((numZeroCrossingsDiagonal,numZeroCrossingsHorizontal),axis=1)\n",
    "numZeroCrossings = np.sum(numZeroCrossings,axis=1)\n",
    "numZeroCrossings = numZeroCrossings[:,np.newaxis]\n",
    "# More strongly weight the number of polynomial curve crossings (weighting of 6)\n",
    "numPolyCrossingsScaled = numPolyCrossings[:,0]*6\n",
    "numPolyCrossingsScaled = numPolyCrossingsScaled[:,np.newaxis]\n",
    "# Combine the number of line crossings (horizontal and diagonal) and the number of \n",
    "# polynomial curve crossings into one summary number\n",
    "numCrossings = numZeroCrossings + numPolyCrossingsScaled\n",
    "# yMaxMinVals and yMaxMinLocs contain two columns, the first with information about\n",
    "# second derivative maxima and the second with information about second derivative \n",
    "# minima. Extract information about only the second derivative maxima\n",
    "yMaxMinVals = yMaxMinVals[:,0]\n",
    "yMaxMinVals = yMaxMinVals[:,np.newaxis]\n",
    "yMaxMinLocs = yMaxMinLocs[:,0]\n",
    "# The range of values for yMaxMinLocs is 1 to ~39. Subtract 1 from all values to comply\n",
    "# with Python indexing which begins at 0. \n",
    "yMaxMinLocs -= 1\n",
    "yMaxMinLocs = yMaxMinLocs[:,np.newaxis]\n",
    "# Instantiate an array to create a new auxiliary input for the value of the amp curve\n",
    "# at the second derivative maxima\n",
    "amp_curve_values_at_second_deriv_max = []\n",
    "# Number of points per curve\n",
    "numPts = 40\n",
    "# Total number of curves\n",
    "numCurves = Derivs_RMS.shape[0]\n",
    "for ii in range(numCurves):\n",
    "    # Get the location (cycle number) of the second derivative maximum for the current\n",
    "    # curve of interest\n",
    "    loc = int(yMaxMinLocs[ii])\n",
    "    # Use all_X.values, which contains the normalized curve values, to obtain the value\n",
    "    # of the amp curve at the second derivative maximum\n",
    "    amp_curve_value_at_second_deriv_max = all_X.values[ii,loc]\n",
    "    amp_curve_values_at_second_deriv_max.append(amp_curve_value_at_second_deriv_max)\n",
    "amp_curve_values_at_second_deriv_max = np.array(amp_curve_values_at_second_deriv_max)\n",
    "amp_curve_values_at_second_deriv_max = amp_curve_values_at_second_deriv_max[:,np.newaxis]\n",
    "# Create a new auxiliary input where the values of the second derivative maxima are \n",
    "# divided by the RMSNoiseScalingFactor to obtain a scaled second derivative maximum value\n",
    "yMaxMinValsScaled = yMaxMinVals/RMSNoiseScalingFactor\n",
    "# Normalize the values of all auxiliary inputs between 0 and 1\n",
    "min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "yMaxMinLocs = min_max_scaler.fit_transform(yMaxMinLocs)\n",
    "min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "numZeroCrossings = min_max_scaler.fit_transform(numZeroCrossings)\n",
    "min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "numPolyCrossings = min_max_scaler.fit_transform(numPolyCrossings)\n",
    "min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "RMSNoiseScalingFactor = min_max_scaler.fit_transform(RMSNoiseScalingFactor)\n",
    "min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "yMaxMinValsScaled = min_max_scaler.fit_transform(yMaxMinValsScaled)\n",
    "\n",
    "# Load in the pre-defined training and test sets. Also load in the\n",
    "# training set indices and test set indices, which define which elements\n",
    "# of all_X and all_Y correspond to the pre-defined training and test sets.\n",
    "\n",
    "X_train = pandas.DataFrame(np.loadtxt(folder_path+'/X_train.txt',delimiter=','))\n",
    "Y_train = np.loadtxt(folder_path+'/Y_train.txt',delimiter=',')\n",
    "X_test = pandas.DataFrame(np.loadtxt(folder_path+'/X_test.txt',delimiter=','))\n",
    "Y_test = pandas.DataFrame(np.loadtxt(folder_path+'/Y_test.txt',delimiter=','))\n",
    "X_train_indices = np.loadtxt(folder_path+'/X_train_indices.txt',delimiter=',')\n",
    "X_train_indices = np.array([int(el) for el in X_train_indices])\n",
    "X_test_indices = np.loadtxt(folder_path+'/X_test_indices.txt',delimiter=',')\n",
    "X_test_indices = np.array([int(el) for el in X_test_indices])\n",
    "\n",
    "# Use the training set indices to extract the correct auxiliary\n",
    "# inputs for the corresponding curves in the training set.\n",
    "X_train_RMSNoise = np.take(RMSNoise,X_train_indices,axis=0)\n",
    "X_train_numZeroCrossings = np.take(numZeroCrossings,X_train_indices,axis=0)\n",
    "X_train_numPolyCrossings = np.take(numPolyCrossings,X_train_indices,axis=0)\n",
    "X_train_yMaxMinVals = np.take(yMaxMinVals,X_train_indices,axis=0)\n",
    "X_train_yMaxMinLocs = np.take(yMaxMinLocs,X_train_indices,axis=0)\n",
    "X_train_amp_curve_values_at_second_deriv_max = np.take(amp_curve_values_at_second_deriv_max,X_train_indices,axis=0)\n",
    "X_train_RMSNoiseScalingFactor = np.take(RMSNoiseScalingFactor,X_train_indices,axis=0)\n",
    "X_train_yMaxMinValsScaled = np.take(yMaxMinValsScaled,X_train_indices,axis=0)\n",
    "X_train_numCrossings = np.take(numCrossings,X_train_indices,axis=0)\n",
    "\n",
    "# Use the test set indices to extract the correct auxiliary\n",
    "# inputs for the corresponding curves in the test set.\n",
    "X_test_RMSNoise = np.take(RMSNoise,X_test_indices,axis=0)\n",
    "X_test_numZeroCrossings = np.take(numZeroCrossings,X_test_indices,axis=0)\n",
    "X_test_numPolyCrossings = np.take(numPolyCrossings,X_test_indices,axis=0)\n",
    "X_test_yMaxMinVals = np.take(yMaxMinVals,X_test_indices,axis=0)\n",
    "X_test_yMaxMinLocs = np.take(yMaxMinLocs,X_test_indices,axis=0)\n",
    "X_test_amp_curve_values_at_second_deriv_max = np.take(amp_curve_values_at_second_deriv_max,X_test_indices,axis=0)\n",
    "X_test_RMSNoiseScalingFactor = np.take(RMSNoiseScalingFactor,X_test_indices,axis=0)\n",
    "X_test_yMaxMinValsScaled = np.take(yMaxMinValsScaled,X_test_indices,axis=0)\n",
    "X_test_numCrossings = np.take(numCrossings,X_test_indices,axis=0)\n",
    "\n",
    "# Create the training and test set auxiliary input arrays\n",
    "X_train_aux_input_1 = X_train_RMSNoise\n",
    "X_test_aux_input_1 = X_test_RMSNoise\n",
    "X_train_aux_input_2 = np.concatenate((X_train_RMSNoise,X_train_numZeroCrossings),axis=1)\n",
    "X_test_aux_input_2 = np.concatenate((X_test_RMSNoise,X_test_numZeroCrossings),axis=1)\n",
    "X_train_aux_input_3 = np.concatenate((X_train_RMSNoise,X_train_numZeroCrossings,X_train_numPolyCrossings),axis=1)\n",
    "X_test_aux_input_3 = np.concatenate((X_test_RMSNoise,X_test_numZeroCrossings,X_test_numPolyCrossings),axis=1)\n",
    "X_train_aux_input_4 = np.concatenate((X_train_RMSNoise,X_train_numZeroCrossings,X_train_numPolyCrossings,X_train_yMaxMinVals),axis=1)\n",
    "X_test_aux_input_4 = np.concatenate((X_test_RMSNoise,X_test_numZeroCrossings,X_test_numPolyCrossings,X_test_yMaxMinVals),axis=1)\n",
    "X_train_aux_input_5 = np.concatenate((X_train_RMSNoise,X_train_numZeroCrossings,X_train_numPolyCrossings,X_train_yMaxMinVals,X_train_yMaxMinLocs),axis=1)\n",
    "X_test_aux_input_5 = np.concatenate((X_test_RMSNoise,X_test_numZeroCrossings,X_test_numPolyCrossings,X_test_yMaxMinVals,X_test_yMaxMinLocs),axis=1)\n",
    "X_train_aux_input_6 = np.concatenate((X_train_RMSNoise,X_train_numZeroCrossings,X_train_numPolyCrossings,X_train_yMaxMinVals,X_train_yMaxMinLocs,X_train_amp_curve_values_at_second_deriv_max),axis=1)\n",
    "X_test_aux_input_6 = np.concatenate((X_test_RMSNoise,X_test_numZeroCrossings,X_test_numPolyCrossings,X_test_yMaxMinVals,X_test_yMaxMinLocs,X_test_amp_curve_values_at_second_deriv_max),axis=1)\n",
    "X_train_aux_input_7 = np.concatenate((X_train_RMSNoise,X_train_numZeroCrossings,X_train_numPolyCrossings,X_train_yMaxMinVals,X_train_yMaxMinLocs,X_train_amp_curve_values_at_second_deriv_max,X_train_RMSNoiseScalingFactor),axis=1)\n",
    "X_test_aux_input_7 = np.concatenate((X_test_RMSNoise,X_test_numZeroCrossings,X_test_numPolyCrossings,X_test_yMaxMinVals,X_test_yMaxMinLocs,X_test_amp_curve_values_at_second_deriv_max,X_test_RMSNoiseScalingFactor),axis=1)\n",
    "X_train_aux_input_8 = np.concatenate((X_train_RMSNoise,X_train_numZeroCrossings,X_train_numPolyCrossings,X_train_yMaxMinVals,X_train_yMaxMinLocs,X_train_amp_curve_values_at_second_deriv_max,X_train_RMSNoiseScalingFactor,X_train_yMaxMinValsScaled),axis=1)\n",
    "X_test_aux_input_8 = np.concatenate((X_test_RMSNoise,X_test_numZeroCrossings,X_test_numPolyCrossings,X_test_yMaxMinVals,X_test_yMaxMinLocs,X_test_amp_curve_values_at_second_deriv_max,X_test_RMSNoiseScalingFactor,X_test_yMaxMinValsScaled),axis=1)\n",
    "\n",
    "# Change the one-hot-encoded training set annotations into single number class\n",
    "# labels \n",
    "Y_train_argmax = np.argmax(Y_train,axis=1)\n",
    "# Use numsamples Stratified K-Fold Cross Validation\n",
    "numsamples = 10\n",
    "skf = StratifiedKFold(n_splits=numsamples)\n",
    "# Create a dictionary holding the training and test set indices for all the\n",
    "# folds in Stratified K-Fold Cross Validation\n",
    "counter = 0\n",
    "cv_dict = dict()\n",
    "for train_index, test_index in skf.split(X_train, Y_train_argmax):\n",
    "    cv_dict[counter] = dict()\n",
    "    cv_dict[counter]['train'] = train_index\n",
    "    cv_dict[counter]['val'] = test_index\n",
    "    counter += 1\n",
    "\n",
    "# Activation Functions to be used \n",
    "softsign = 'softsign'\n",
    "relu = 'relu'\n",
    "tanh = 'tanh'\n",
    "sigmoid = 'sigmoid'\n",
    "linear = 'linear'\n",
    "\n",
    "# Optimizer to be used\n",
    "sgd = SGD(lr=0.1,decay=0.005,nesterov=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 - Parameter Toolbox\n",
    "\n",
    "The next few cells will guide you through choosing the parameters of your neural network. As a first pass to get a feeling for how the model is working, you can run the cells below keeping the default values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Number of neurons per hidden layer\n",
    "\n",
    "<img src=\"nn-architecture.png\" style=\"width:350px;height:270px;\">\n",
    "\n",
    "The architecture we will be implementing consists of one input layer (not shown), 3 hidden layers, and one output layer as depicted above. \n",
    "\n",
    "- **Input Layer (Not shown):** Default (Unchangeable) of 40. 40 neurons for the 40-dimensional input (40 normalized fluorescence values for every amplification curve in the dataset).  \n",
    "\n",
    "- **First Hidden Layer:** Default of 30. Every neuron in this layer is \"fully connected\" to every neuron in the input layer. Learns the interrelationships and interdependencies between all the input values. \n",
    "\n",
    "- **Second Hidden Layer:** Default of 30. Every neuron in this layer is \"fully connected\" to every neuron in the first hidden layer. The second hidden layer learns more abstract, latent features produced by the first hidden layer. \n",
    "\n",
    "- **Third Hidden Layer:**: Default of 20. Every neuron in this layer is \"fully connected\" to every neuron in the second hidden layer. The third hidden layer learns even more abstract, latent features produced by the second hidden layer. Typically this layer will contain less neurons than the second and first hidden layers, as the model more finely squeezes information out of the inputs. \n",
    "\n",
    "- **Output Layer:** Default (Unchangeable) of 3. 3 neurons for the 3-dimensional output (3 probabilities for each of the three classes: -1 or Non-Amplified, 0 or Inconclusive, and 1 or Amplified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change (or leave as default) the values in the cell below to experiment with the number of neurons in each layer of the architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of neurons in hidden layer 1\n",
    "layer1_depth = 30\n",
    "\n",
    "# Number of neurons in hidden layer 2\n",
    "layer2_depth = 30\n",
    "\n",
    "# Number of neurons in hidden layer 3\n",
    "layer3_depth = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Activation Functions per Hidden Layer\n",
    "\n",
    "<img src=\"activation-functions.png\" style=\"width:350px;height:300px;\">\n",
    "\n",
    "As demonstrated by the diagram above, after each neuron in a layer outputs its raw value, this value is typically passed through an activation function. This activation function adds an additional nonlinearity, allowing the network to learn more nonlinear mappings. A few activation functions are displayed below:\n",
    "\n",
    "<img src=\"tanh_softsign.png\" style=\"width:350px;height:200px;\">\n",
    "<img src=\"sigmoid.png\" style=\"width:300px;height:100px;\">\n",
    "<img src=\"relu.png\" style=\"width:300px;height:100px;\">\n",
    "<img src=\"linear.png\" style=\"width:250px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change (or leave as default) the values in the cell below to experiment with the activation functions in each layer of the architecture. The options you have are:\n",
    "     - tanh\n",
    "     - softsign\n",
    "     - sigmoid\n",
    "     - relu\n",
    "     - linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Activation function in hidden layer 1\n",
    "layer1_activation = relu\n",
    "\n",
    "# Activation function in hidden layer 2\n",
    "layer2_activation = relu\n",
    "\n",
    "# Activation function in hidden layer 3\n",
    "layer3_activation = relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Batch Size and Number of Epochs\n",
    "\n",
    "The last parameters we will be changing before creating and fitting our model are the batch size and number of epochs:\n",
    "\n",
    "- **Batch Size:** Default of 200. The number of samples in your dataset to give to the neural network before it computes the loss, backpropagates the error, and updates its parameters.\n",
    "    - Increasing the batch size lets the network run in shorter computation time. As the network learns, larger batch sizes may also prevent oscillations about local minima due to computing the loss over a larger number of samples instead of (in the extreme case) one sample at a time.\n",
    "    - Decreasing the batch size allows the network to update its parameters more frequently. \n",
    "\n",
    "- **Number of Epochs:** Default of 10. The number of times the neural network will pass through the entire dataset. \n",
    "    - Increasing the number of epochs will let the network learn the training set very well. Increasing the number too far may lead to overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change (or leave as default) the values in the cell below to experiment with the batch size and number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch Size\n",
    "batch_size = 200\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 - Creating and Fitting the Neural Network\n",
    "\n",
    "Keras is an excellent framework for rapidly prototyping deep learning models.\n",
    "\n",
    "Here is an example of how to create a neural network in Keras:\n",
    "\n",
    "```python\n",
    "# Uses the Sequential Model API in which every layer is linearly stacked on top of the last\n",
    "model = Sequential()\n",
    "# To add a layer, simply use the model.add() function\n",
    "# Here we are adding a Dense (fully connected hidden layer) to the network with the number\n",
    "# of neurons (\"layer1_depth\") and activation function (\"layer1_activation\") you specified \n",
    "# earlier. \n",
    "# Note that for the first layer in the Sequential Model, you must specify the dimensionality\n",
    "# of your inputs. After this point, Keras will automatically calculate input and output shapes\n",
    "# for you. \n",
    "model.add(Dense(layer1_depth, activation=layer1_activation, input_dim=40))\n",
    "# Add the second hidden layer \n",
    "model.add(Dense(layer2_depth,activation=layer2_activation))\n",
    "# Add the third hidden layer\n",
    "model.add(Dense(layer3_depth,activation=layer3_activation))\n",
    "# The last layer will be the output layer and must contain 3 neurons for the 3 class \n",
    "# probabilities we want to output. The \"softmax\" activation function is also necessary\n",
    "# to ensure these probabilities sum to 1. \n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# Compile the model, preparing it to be fit to the training data. Leave the parameters \n",
    "# at the defaults for now. \n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In only six lines, you've created a neural network model. Run the following cell to create your neural network and fit it on the training set:\n",
    "\n",
    "*Note that there are additional BatchNormalization layers after each Dense layer. These BatchNormalization layers normalize the distribution of the output values after each Dense layer, improving network performance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Neural Network model\n",
    "model = Sequential()\n",
    "model.add(Dense(layer1_depth, activation=layer1_activation, input_dim=40))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(layer2_depth,activation=layer2_activation))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(layer3_depth,activation=layer3_activation))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit the model on the training set for the batch size and number of epochs you specified earlier\n",
    "model.fit(X_train,Y_train,epochs=epochs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've successfully created and fit your model, run the following cell to see the false positive rate (FPR), false negative rate (FNR), true positive rate (TPR), and true negative rate (TNR) performance metrics on the training set. A confusion matrix is also displayed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert array of annotations (Y_train) from Keras one-hot encoding to int labels (0,1,2)\n",
    "Y_train_argmax = np.argmax(Y_train,axis=1)\n",
    "# Obtain predictions from model on training set \n",
    "Y_pred = model.predict(X_train)\n",
    "# Convert probabilities into class labels \n",
    "Y_pred = Y_pred.argmax(axis=-1)\n",
    "# Compute performance metrics\n",
    "df_conf_mat,fpr,fnr,tpr,tnr,inc_pos,inc_inc,inc_neg,pos_inc,neg_inc = compute_confmat_3classes(Y_pred, Y_train_argmax)\n",
    "print('FPR: '+str(fpr*100))\n",
    "print('FNR: '+str(fnr*100))\n",
    "print('TPR: '+str(tpr*100))\n",
    "print('TNR: '+str(tnr*100))\n",
    "df_conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to see the FPR, FNR, TPR, and TNR performance metrics, along with the confusion matrix for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obtain predictions from model on test set \n",
    "Y_pred = model.predict(X_test)\n",
    "# Convert probabilities into class labels \n",
    "Y_pred = Y_pred.argmax(axis=-1)\n",
    "# Compute performance metrics\n",
    "df_conf_mat,fpr,fnr,tpr,tnr,inc_pos,inc_inc,inc_neg,pos_inc,neg_inc = compute_confmat_3classes(Y_pred, Y_test)\n",
    "print('FPR: '+str(fpr*100))\n",
    "print('FNR: '+str(fnr*100))\n",
    "print('TPR: '+str(tpr*100))\n",
    "print('TNR: '+str(tnr*100))\n",
    "df_conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 - Adding Auxiliary Inputs\n",
    "\n",
    "Now, to improve performance, you can add auxiliary inputs to the model. These features were hand-engineered to give the network additional information it can use to make predictions. The auxiliary inputs are as follows:\n",
    "\n",
    "- **RMS Noise:** Approximate RMS Noise in the curve, calculated by subtracting the normalized curve from a smoothed version of the normalized curve and finding the sum of squares of the residual. \n",
    "\n",
    "- **Number of Line Crossings:** Total number of crossings between the curve and various horizontal and diagonal lines with different slopes and intercept. \n",
    "\n",
    "- **Number of Polynomial Curve Crossings:** Total number of crossings between the curve and a 4th order polynomial fit to the curve. \n",
    "\n",
    "- **Second Derivative Max Value:**: Value of the second derivative maximum of the curve. \n",
    "\n",
    "- **Second Derivative Max Location:** Cycle number at which the second derivative maximum occurs.\n",
    "\n",
    "- **Amp Curve Value at Second Derivative Max:** Value of the normalized amp curve at the second derivative max\n",
    "\n",
    "- **RMS Noise Scaling Factor:** Scaling factor used to scale the second derivative max value. The RMS Noise Scaling factor is larger for noisier curves. \n",
    "\n",
    "- **Scaled Second Derivative Max Value:** Second derivative max value divided by the RMS Noise Scaling Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the number of auxiliary inputs you wish to add to the model. The options you have are:\n",
    "     - 1: RMS Noise\n",
    "     - 2: (Same as 1) + Number of Line Crossings\n",
    "     - 3: (Same as 2) + Number of Polynomial Curve Crossings\n",
    "     - 4: (Same as 3) + Second Derivative Max Value\n",
    "     - 5: (Same as 4) + Second Derivative Max Location\n",
    "     - 6: (Same as 5) + Amp Curve Value at Second Derivative Max\n",
    "     - 7: (Same as 6) + RMS Noise Scaling Factor\n",
    "     - 8: (Same as 7) + Scaled Second Derivative Max Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the argument of \"num_aux_input\" below to the number of auxiliary inputs you wish to add: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The number of auxiliary inputs to add. Choose from [1,2,3,4,5,6,7,8]:\n",
    "num_aux_input = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, you can change (or leave as default) the values in the cell below, which specify the parameters of the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of neurons per hidden layer\n",
    "layer1_depth = 30\n",
    "layer2_depth = 30\n",
    "layer3_depth = 20\n",
    "\n",
    "# Activation Functions per hidden layer\n",
    "layer1_activation = relu\n",
    "layer2_activation = relu\n",
    "layer3_activation = relu\n",
    "\n",
    "# Batch size and Number of epochs\n",
    "batch_size = 200\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Regularization\n",
    "\n",
    "An additional modification that can improve performance is the incorporation of reguarlization into the loss function:\n",
    "\n",
    "<img src=\"regularization.png\" style=\"width:350px;height:100px;\">\n",
    "\n",
    "\"C0\" is the categorical cross entropy loss function used by the model we are working with, \"n\" is the number of samples in the training set, and \"w\" are the weights learned by the network. Regularization serves to penalize larger weights, effectively making the model simpler and less prone to overfitting. The amount of regularization is determined by lambda (regularization parameter) in the equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change (or leave as default) the regularization parameter (lambda) in the cell below. Keep in mind that due to the way it is incorporated in the loss function, the regularization parameter (lambda) determines a tradeoff between learning small weights and minimizing the original loss function (categorical cross entropy). \n",
    "    - Increasing lambda increases the emphasis on learning small weights\n",
    "    - Decreasing lambda increases the emphasis on minimizing the original loss function. \n",
    "It is suggested to start with small values of lambda, such as the default of 0.01, to incorporate the benefits of regularization (increasing generalization capability) as well as the benefits of the original loss function (learning the training set well). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regularization parameter (lambda)\n",
    "regularization = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Class Weighting\n",
    "\n",
    "The last modification we will make to the loss function is the addition of class weighting.\n",
    "\n",
    "<img src=\"class_weighting_loss.png\" style=\"width:350px;height:150px;\">\n",
    "<img src=\"class_weighting_formula.png\" style=\"width:350px;height:200px;\">\n",
    "\n",
    "\"Li\" is the loss function for sample i, \"n\" is the total number of samples in the training set, \"ni\" is the number of samples in class i, and \"c\" is the total number of classes.\n",
    "\n",
    "- **wi:** Weight applied to the loss for each sample in class i. In order to address the skewed distribution in the dataset, higher weights are given to more poorly-represented classes, informing the network that those classes contain the greatest source of information.\n",
    "\n",
    "- **alpha:** Weighting exponent that determines how aggressively to emphasize the more poorly-represented classes as sources of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change (or leave as default) the alpha parameter in the cell below. \n",
    "    - Increasing alpha, for example, increases the weight given to the Non-Amplified (-1) class. This more heaviliy penalizes misclassifications from this class (i.e. when an example annotated as -1 is called 1), thereby decreasing the false positive rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alpha parameter in the class weighting formula\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 - Creating and Fitting the Modified Model\n",
    "\n",
    "The Sequential API was perfect for the relatively simple models we implemented thus far in the tutorial because those models were all \"sequential,\" in other words one layer's output became the next layer's input and so on. For example: \n",
    "\n",
    "<img src=\"visualization_pictures/sequential_api.png\" style=\"width:350px;height:400px;\">\n",
    "\n",
    "However, now that we wish to add auxiliary inputs to our model, the model can no longer be compiled sequentially and we must turn instead to Keras' Functional API. Keras' Functional API has an incredible range of flexibility in model development, allowing you now to create models that accept multiple inputs: \n",
    "\n",
    "<img src=\"visualization_pictures/functional_api.png\" style=\"width:500px;height:300px;\">\n",
    "\n",
    "At more advanced stages you can even create models with multiple inputs and multiple outputs, with layers recombining at different stages: \n",
    "\n",
    "<img src=\"visualization_pictures/functional_api_complex.png\" style=\"width:400px;height:700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the added benefit of flexibiltiy, Keras' Functional API is only slightly more complex than the Sequential API. Let's dive in to the syntax. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we created the following neural network via the Sequential API, repeated below for convenience. The line numbers will be labeled in order for future reference. \n",
    "\n",
    "```python\n",
    "# Line 0\n",
    "model = Sequential()\n",
    "# Line 1\n",
    "model.add(Dense(layer1_depth, activation=layer1_activation, input_dim=40))\n",
    "# Line 2\n",
    "model.add(Dense(layer2_depth,activation=layer2_activation))\n",
    "# Line 3\n",
    "model.add(Dense(layer3_depth,activation=layer3_activation))\n",
    "# Line 4\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# Line 5\n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "Let's jump straight into the Functional API translation of the same model. Note that line numbers here are not in order, but refer to the corresponding line numbers in the Sequential API code:\n",
    "\n",
    "```python\n",
    "# Line 1a\n",
    "inputs = Input(shape=(40,))\n",
    "# Line 1b\n",
    "x = Dense(layer1_depth, activation=layer1_activation)(inputs)\n",
    "# Line 2\n",
    "x = Dense(layer2_depth, activation=layer2_activation)(x)\n",
    "# Line 3\n",
    "x = Dense(layer3_depth, activation=layer3_activation)(x)\n",
    "# Line 4\n",
    "outputs = Dense(3, activation='softmax')(x)\n",
    "# Line 0\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "# Line 5\n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "Now for the details. The major difference between the Sequential and Functional API is how layers are *added* to the model. In both cases, a layer is *created* the same way - by using the syntax typically reserved for classes in Python (indicated in blue font). For example, to create a Dense layer you use: \n",
    "\n",
    "<span style=\"color:blue\">Dense(layer_depth, activation=layer_activation)</span> \n",
    "\n",
    "- In the Sequential API, this layer object is then added to the model via the add() function. The Sequential API-specific syntax is indicated in red font:<br><br>\n",
    "<span style=\"color:red\">model.add(</span><span style=\"color:blue\">Dense(layer_depth,activation=layer_activation)</span><span style=\"color:red\">)</span><br><br>\n",
    "- In the Functional API, this layer object is stored as a variable, which can be regarded as the layer's \"name.\" Layers no longer have to be added sequentially because these layer \"names\" can be used to reference and connect layers anywhere within the model. The Functional API-specific syntax is indicated in red font: <br><br>\n",
    "<span style=\"color:red\">x = </span><span style=\"color:blue\">Dense(layer_depth,activation=layer_activation)</span><span style=\"color:red\">(inputs)</span><br><br>\n",
    "Here, <span style=\"color:red\">\"x\"</span> is the name of the Dense layer we have created and <span style=\"color:red\">\"inputs\"</span> is the name of the layer we want to connect our new Dense layer to (i.e. \"inputs\" precedes our new Dense layer in the model architecture).\n",
    "\n",
    "Let's now proceed through a line-by-line comparison between the two APIs: <br>\n",
    "- **Line 0**: Line 0 in the Sequential API immediately instantiates the model. By contrast, in the Functional API, instantiation of the model occurs in the second-to-last line after all of the layers are defined. To instantiate the model, call the Model() function and pass it two arguments:\n",
    "    - The first argument \"inputs\" is the name of the Input Layer (to be discussed)\n",
    "    - The second argument \"outputs\" is the name of the Output Layer (to be discussed)\n",
    "- **Line 1**: In the Sequential API, Line 1 creates and adds the first Dense layer to the model. Note that Line 1 must also specify an argument \"input_dim,\" which informs the model what dimensionality to expect from the input vectors. (In this case, the input vectors are 40-dimensional for the 40 cycles of the amp curves). In the Functional API, Line 1 is split into two parts: \n",
    "    - Line 1a informs the model what dimensionality to expect from the input vectors by creating an Input Layer. The \"shape\" argument to the Input Layer replaces the \"input_dim\" argument in the Sequential API. (Note that the \",\" is necessary after the 40 in order to indicate there are no additional dimensions). The Input layer is saved in the variable \"inputs.\" \n",
    "    - Line 1b creates the first Dense layer in the model, saves it in the variable \"x,\" connects it to the layer \"inputs,\" which is the name of our Input Layer from Line 1a. \n",
    "- **Line 2**: In the Sequential API, Line 2 creates and adds the second Dense layer to the model. Note that the \"input_dim\" argument is no longer necessary because Keras can infer the input dimensionalities of all layers after the first. In the Functional API, Line 3 creates the second Dense layer in the model, saves it in the variable \"x,\" and connects it to the layer \"x,\" which was the name of the first Dense layer in the model. \n",
    "    - Note that we are overriding the \"x\" variable every time we create a new layer. This is completely fine for our purposes and for simplicity. However, for more advanced uses it is recommended that you create unique variable names for each layer in your model. \n",
    "- **Line 3**: In the Sequential API, Line 3 creates and adds the third Dense layer to the model. In the Functional API, Line 3 creates the third Dense layer in the model, saves it in the variable \"x,\" and connects it to the layer \"x,\" which was the name of the second Dense layer in the model. \n",
    "- **Line 4**: In the Sequential API, Line 4 creates and adds the fourth Dense layer to the model. In the Functional API, Line 4 creates the fourth Dense layer in the model, saves it in the variable \"x,\" and connects it to the layer \"x,\" which was the name of the third Dense layer in the model. \n",
    "- **Line 5**: Line 5 is identical in the Sequential API and Functional API, its purpose being to compile our completed model using the SGD optimizer, categorical-cross-entropy loss, and accuracy metric. The details of these parameters are unimportant at this time. It is, however, important to note that all three arguments must be specified in the compile() function for models in this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Keras' Functional API to create our modified model, which now incorporates auxiliary inputs, regularization, and class weighting. Run the cell below to create and fit the model on the training set. If you wish to read through the details of the code below, make note of four important points:\n",
    "1. *As before in the Sequential model, there are additional BatchNormalization layers after each Dense layer, which normalize the distribution of the output values after each Dense layer, improving network performance.*\n",
    "2. *For clarity, layer names are explicitly specified with strings using the \"name\" parameter when creating layers.*\n",
    "3. *When instantiating the model using the Model() function, the \"inputs\" parameter can accept an array, the elements of which are all the desired Input layers of the model.*\n",
    "4. *If the model was instantiated using an array for the \"inputs\" parameter in Model(), it must **also** be fit using an array of inputs of the same size. The first element of the input given to fit() corresponds to the first layer in the array given to the \"inputs\" parameter of Model(), the second element of the input given to fit() corresponds to the second layer in the array given to the \"inputs\" parameter of Model(), and so on.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Neural Network Model\n",
    "X_input = Input(shape=(40,),name='Input')\n",
    "X_dense1 = Dense(layer1_depth, activation=relu, kernel_regularizer=regularizers.l2(regularization), name='Dense_1')(X_input)\n",
    "X_bn1 = BatchNormalization(name='BatchNorm_1')(X_dense1)\n",
    "X_dense2 = Dense(layer2_depth, activation=relu, kernel_regularizer=regularizers.l2(regularization), name='Dense_2')(X_bn1)\n",
    "X_bn2 = BatchNormalization(name='BatchNorm_2')(X_dense2)\n",
    "aux_input = Input(shape=(num_aux_input,), name='aux_input')\n",
    "X_concat = keras.layers.concatenate([X_bn2,aux_input],name='concat')\n",
    "X_bn3 = BatchNormalization(name='BatchNorm_3')(X_concat)\n",
    "X_dense3 = Dense(layer3_depth, activation=relu, kernel_regularizer=regularizers.l2(regularization), name='Dense_3')(X_bn3)\n",
    "X_bn4 = BatchNormalization(name='BatchNorm_4')(X_dense3)\n",
    "X_output = Dense(3, activation='softmax', name='Output')(X_bn4)\n",
    "model = Model(inputs=[X_input,aux_input],outputs=X_output)\n",
    "sgd = SGD(lr=0.1,decay=0.005,nesterov=False)\n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Determine which auxiliary inputs to use based on the user-specified \"num_aux_inputs\"\n",
    "if num_aux_input == 1:\n",
    "    X_train_aux_input = X_train_aux_input_1\n",
    "    X_test_aux_input = X_test_aux_input_1\n",
    "elif num_aux_input == 2:\n",
    "    X_train_aux_input = X_train_aux_input_2\n",
    "    X_test_aux_input = X_test_aux_input_2\n",
    "elif num_aux_input == 3:\n",
    "    X_train_aux_input = X_train_aux_input_3\n",
    "    X_test_aux_input = X_test_aux_input_3\n",
    "elif num_aux_input == 4:\n",
    "    X_train_aux_input = X_train_aux_input_4\n",
    "    X_test_aux_input = X_test_aux_input_4 \n",
    "elif num_aux_input == 5:\n",
    "    X_train_aux_input = X_train_aux_input_5\n",
    "    X_test_aux_input = X_test_aux_input_5 \n",
    "elif num_aux_input == 6:\n",
    "    X_train_aux_input = X_train_aux_input_6\n",
    "    X_test_aux_input = X_test_aux_input_6\n",
    "elif num_aux_input == 7:\n",
    "    X_train_aux_input = X_train_aux_input_7\n",
    "    X_test_aux_input = X_test_aux_input_7\n",
    "elif num_aux_input == 8:\n",
    "    X_train_aux_input = X_train_aux_input_8\n",
    "    X_test_aux_input = X_test_aux_input_8\n",
    "\n",
    "# Determine class weight dictionary based on user-specified \"alpha\"\n",
    "# \"fracts\" is the distribution of classes in our dataset, indicating that ~16% of the dataset is non-amplified,\n",
    "# ~1% is inconclusive, and ~83% is amplified.\n",
    "fracts = np.array([0.16,0.01,0.83])\n",
    "invFracts = np.power((1/fracts),alpha)\n",
    "weights = invFracts/(np.sum(invFracts))\n",
    "class_weight_dict = {0:weights[0],1:weights[1],2:weights[2]}\n",
    "\n",
    "# Fit the model\n",
    "model.fit([X_train,X_train_aux_input],Y_train,epochs=epochs,batch_size=batch_size,class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've successfully created and fit your model, run the following cell to see the false positive rate (FPR), false negative rate (FNR), true positive rate (TPR), and true negative rate (TNR) performance metrics on the training set. A confusion matrix is also displayed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert array of annotations (Y_train) from Keras one-hot encoding to int labels (0,1,2)\n",
    "Y_train_argmax = np.argmax(Y_train,axis=1)\n",
    "# Obtain predictions from model on training set \n",
    "Y_pred = model.predict([X_train,X_train_aux_input])\n",
    "# Convert probabilities into class labels \n",
    "Y_pred = Y_pred.argmax(axis=-1)\n",
    "# Compute performance metrics\n",
    "df_conf_mat,fpr,fnr,tpr,tnr,inc_pos,inc_inc,inc_neg,pos_inc,neg_inc = compute_confmat_3classes(Y_pred, Y_train_argmax)\n",
    "print('FPR: '+str(fpr*100))\n",
    "print('FNR: '+str(fnr*100))\n",
    "print('TPR: '+str(tpr*100))\n",
    "print('TNR: '+str(tnr*100))\n",
    "df_conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to see the FPR, FNR, TPR, and TNR performance metrics, along with the confusion matrix for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obtain predictions from model on test set \n",
    "Y_pred = model.predict([X_test,X_test_aux_input])\n",
    "# Convert probabilities into class labels \n",
    "Y_pred = Y_pred.argmax(axis=-1)\n",
    "# Compute performance metrics\n",
    "df_conf_mat,fpr,fnr,tpr,tnr,inc_pos,inc_inc,inc_neg,pos_inc,neg_inc = compute_confmat_3classes(Y_pred, Y_test)\n",
    "print('FPR: '+str(fpr*100))\n",
    "print('FNR: '+str(fnr*100))\n",
    "print('TPR: '+str(tpr*100))\n",
    "print('TNR: '+str(tnr*100))\n",
    "df_conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 - Experiment\n",
    "\n",
    "Now that you have a feel for how the neural networks work, all of the default parameters are listed in the cell below. Adjust them, experimenting with different combinations of parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of auxiliary inputs\n",
    "num_aux_input = 6\n",
    "# Number of neurons per hidden layer\n",
    "layer1_depth = 30\n",
    "layer2_depth = 30\n",
    "layer3_depth = 20\n",
    "# Activation Functions per hidden layer\n",
    "layer1_activation = relu\n",
    "layer2_activation = relu\n",
    "layer3_activation = relu\n",
    "# Batch size and Number of epochs\n",
    "batch_size = 200\n",
    "epochs = 10\n",
    "# Regularization parameter (lambda)\n",
    "regularization = 0.01\n",
    "# alpha parameter in the class weighting formula\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the next three cells to create and fit the model, obtain performance metrics on the training set, and obtain performance metrics on the test set. Go back and repeat the steps for this section (5.0) as many times as you wish before proceeding to the final section (6.0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Neural Network Model\n",
    "X_input = Input(shape=(40,),name='Input')\n",
    "X_dense1 = Dense(layer1_depth, activation=relu, kernel_regularizer=regularizers.l2(regularization), name='Dense_1')(X_input)\n",
    "X_bn1 = BatchNormalization(name='BatchNorm_1')(X_dense1)\n",
    "X_dense2 = Dense(layer2_depth, activation=relu, kernel_regularizer=regularizers.l2(regularization), name='Dense_2')(X_bn1)\n",
    "X_bn2 = BatchNormalization(name='BatchNorm_2')(X_dense2)\n",
    "aux_input = Input(shape=(num_aux_input,), name='aux_input')\n",
    "X_concat = keras.layers.concatenate([X_bn2,aux_input],name='concat')\n",
    "X_bn3 = BatchNormalization(name='BatchNorm_3')(X_concat)\n",
    "X_dense3 = Dense(layer3_depth, activation=relu, kernel_regularizer=regularizers.l2(regularization), name='Dense_3')(X_bn3)\n",
    "X_bn4 = BatchNormalization(name='BatchNorm_4')(X_dense3)\n",
    "X_output = Dense(3, activation='softmax', name='Output')(X_bn4)\n",
    "model = Model(inputs=[X_input,aux_input],outputs=X_output)\n",
    "sgd = SGD(lr=0.1,decay=0.005,nesterov=False)\n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# Determine which auxiliary inputs to use based on the user-specified \"num_aux_inputs\"\n",
    "if num_aux_input == 1:\n",
    "    X_train_aux_input = X_train_aux_input_1\n",
    "    X_test_aux_input = X_test_aux_input_1\n",
    "elif num_aux_input == 2:\n",
    "    X_train_aux_input = X_train_aux_input_2\n",
    "    X_test_aux_input = X_test_aux_input_2\n",
    "elif num_aux_input == 3:\n",
    "    X_train_aux_input = X_train_aux_input_3\n",
    "    X_test_aux_input = X_test_aux_input_3\n",
    "elif num_aux_input == 4:\n",
    "    X_train_aux_input = X_train_aux_input_4\n",
    "    X_test_aux_input = X_test_aux_input_4 \n",
    "elif num_aux_input == 5:\n",
    "    X_train_aux_input = X_train_aux_input_5\n",
    "    X_test_aux_input = X_test_aux_input_5 \n",
    "elif num_aux_input == 6:\n",
    "    X_train_aux_input = X_train_aux_input_6\n",
    "    X_test_aux_input = X_test_aux_input_6\n",
    "elif num_aux_input == 7:\n",
    "    X_train_aux_input = X_train_aux_input_7\n",
    "    X_test_aux_input = X_test_aux_input_7\n",
    "elif num_aux_input == 8:\n",
    "    X_train_aux_input = X_train_aux_input_8\n",
    "    X_test_aux_input = X_test_aux_input_8\n",
    "\n",
    "# Determine class weight dictionary based on user-specified \"alpha\"\n",
    "fracts = np.array([0.16,0.01,0.83])\n",
    "invFracts = np.power((1/fracts),alpha)\n",
    "weights = invFracts/(np.sum(invFracts))\n",
    "class_weight_dict = {0:weights[0],1:weights[1],2:weights[2]}\n",
    "\n",
    "# Fit the model\n",
    "model.fit([X_train,X_train_aux_input],Y_train,epochs=epochs,batch_size=batch_size,class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert array of annotations (Y_train) from Keras one-hot encoding to int labels (0,1,2)\n",
    "Y_train_argmax = np.argmax(Y_train,axis=1)\n",
    "# Obtain predictions from model on training set \n",
    "Y_pred = model.predict([X_train,X_train_aux_input])\n",
    "# Convert probabilities into class labels \n",
    "Y_pred = Y_pred.argmax(axis=-1)\n",
    "# Compute performance metrics\n",
    "df_conf_mat,fpr,fnr,tpr,tnr,inc_pos,inc_inc,inc_neg,pos_inc,neg_inc = compute_confmat_3classes(Y_pred, Y_train_argmax)\n",
    "print('FPR: '+str(fpr*100))\n",
    "print('FNR: '+str(fnr*100))\n",
    "print('TPR: '+str(tpr*100))\n",
    "print('TNR: '+str(tnr*100))\n",
    "df_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obtain predictions from model on test set \n",
    "Y_pred = model.predict([X_test,X_test_aux_input])\n",
    "# Convert probabilities into class labels \n",
    "Y_pred = Y_pred.argmax(axis=-1)\n",
    "# Compute performance metrics\n",
    "df_conf_mat,fpr,fnr,tpr,tnr,inc_pos,inc_inc,inc_neg,pos_inc,neg_inc = compute_confmat_3classes(Y_pred, Y_test)\n",
    "print('FPR: '+str(fpr*100))\n",
    "print('FNR: '+str(fnr*100))\n",
    "print('TPR: '+str(tpr*100))\n",
    "print('TNR: '+str(tnr*100))\n",
    "df_conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 - Obtaining Performance Statistics\n",
    "\n",
    "Now that you've created, fit, and determined the performance metrics for one iteration of your model, you can create several iterations to obtain performance statistics (mean, standard deviation). The cell below will perform the following steps:\n",
    "\n",
    "- **1. Instantiate Model:** Your model architecture and initialized weights will be saved to an H5 file. This file represents the base version of the model prior to any fitting on the dataset and will be called each time we create a new iteration of the model. \n",
    "\n",
    "- **2. Stratified K-Fold Partitioning:** The training set will be stratified into K folds, such that K-1 folds are used to train the model and the last, unused fold is used for validation. \"Stratified\" refers to the class distribution in the entire training set being preserved in each fold.\n",
    "\n",
    "- **3. Cross Validation and Testing:** The stratified K-Fold partitioning is repeated 10 times and the mean and standard deviation of the performance metrics over all 10 runs computed. Performance metrics are computed, for each run, on the training set and the validation set. Note that the training and validation set change during each run, so that the model eventually encounters every example in the training set. The test set, which the model never encountered during any run of training, is then predicted on to obtain performance metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to run the stratified K-Fold cross validation procedure. For simplicity, the code below will display performance metrics (FPR, FNR, TPR, TNR) on only the training and test set. Note that this cell will take a few minutes to run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine which auxiliary inputs to use based on the user-specified \"num_aux_inputs\"\n",
    "aux_dict = dict()\n",
    "if num_aux_input == 1:\n",
    "    aux_dict['X_train_aux_input'] = X_train_aux_input_1\n",
    "    aux_dict['X_test_aux_input'] = X_test_aux_input_1\n",
    "elif num_aux_input == 2:\n",
    "    aux_dict['X_train_aux_input'] = X_train_aux_input_2\n",
    "    aux_dict['X_test_aux_input'] = X_test_aux_input_2\n",
    "elif num_aux_input == 3:\n",
    "    aux_dict['X_train_aux_input'] = X_train_aux_input_3\n",
    "    aux_dict['X_test_aux_input'] = X_test_aux_input_3\n",
    "elif num_aux_input == 4:\n",
    "    aux_dict['X_train_aux_input'] = X_train_aux_input_4\n",
    "    aux_dict['X_test_aux_input'] = X_test_aux_input_4 \n",
    "elif num_aux_input == 5:\n",
    "    aux_dict['X_train_aux_input'] = X_train_aux_input_5\n",
    "    aux_dict['X_test_aux_input'] = X_test_aux_input_5 \n",
    "elif num_aux_input == 6:\n",
    "    aux_dict['X_train_aux_input'] = X_train_aux_input_6\n",
    "    aux_dict['X_test_aux_input'] = X_test_aux_input_6\n",
    "elif num_aux_input == 7:\n",
    "    aux_dict['X_train_aux_input'] = X_train_aux_input_7\n",
    "    aux_dict['X_test_aux_input'] = X_test_aux_input_7\n",
    "elif num_aux_input == 8:\n",
    "    aux_dict['X_train_aux_input'] = X_train_aux_input_8\n",
    "    aux_dict['X_test_aux_input'] = X_test_aux_input_8\n",
    "\n",
    "# Determine class weight dictionary based on user-specified \"alpha\"\n",
    "class_weight_dict = dict()\n",
    "fracts = np.array([0.16,0.01,0.83])\n",
    "invFracts = np.power((1/fracts),alpha)\n",
    "weights = invFracts/(np.sum(invFracts))\n",
    "class_weight_dict = {0:weights[0],1:weights[1],2:weights[2]}\n",
    "\n",
    "# Compute performance metric statistics for your Neural Network \n",
    "model_name = 'amp_model_demo.h5'\n",
    "model.save(model_name)\n",
    "all_dict = cv_trainingset(model_name,X_train.values,Y_train,X_test.values,Y_test.values,numsamples,cv_dict,aux_dict,class_weight_dict)\n",
    "\n",
    "print('Training Set Performance Metrics')\n",
    "print(\"FPR: %0.2f (+/- %0.2f)\" % (all_dict['fpr_train'][0]*100, all_dict['fpr_train'][1]*100 * 2))\n",
    "print(\"FNR: %0.2f (+/- %0.2f)\" % (all_dict['fnr_train'][0]*100, all_dict['fnr_train'][1]*100 * 2))\n",
    "print(\"TPR: %0.2f (+/- %0.2f)\" % (all_dict['tpr_train'][0]*100, all_dict['tpr_train'][1]*100 * 2))\n",
    "print(\"TNR: %0.2f (+/- %0.2f)\" % (all_dict['tnr_train'][0]*100, all_dict['tnr_train'][1]*100 * 2))\n",
    "\n",
    "print('Test Set Performance Metrics')\n",
    "print(\"FPR: %0.2f (+/- %0.2f)\" % (all_dict['fpr_test'][0]*100, all_dict['fpr_test'][1]*100 * 2))\n",
    "print(\"FNR: %0.2f (+/- %0.2f)\" % (all_dict['fnr_test'][0]*100, all_dict['fnr_test'][1]*100 * 2))\n",
    "print(\"TPR: %0.2f (+/- %0.2f)\" % (all_dict['tpr_test'][0]*100, all_dict['tpr_test'][1]*100 * 2))\n",
    "print(\"TNR: %0.2f (+/- %0.2f)\" % (all_dict['tnr_test'][0]*100, all_dict['tnr_test'][1]*100 * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the cell below and compare the results you get with the table in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set parameters \n",
    "num_aux_input = 8\n",
    "regularization = 0.01\n",
    "alpha = 1.2\n",
    "layer1_depth = 30\n",
    "layer2_depth = 30\n",
    "layer3_depth = 20\n",
    "layer1_activation = relu\n",
    "layer2_activation = relu\n",
    "layer3_activation = relu\n",
    "batch_size = 200\n",
    "epochs = 10\n",
    "\n",
    "# Set random seeds\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "# Create model\n",
    "X_input = Input(shape=(40,),name='Input')\n",
    "X_dense1 = Dense(layer1_depth, activation=relu, kernel_regularizer=regularizers.l2(regularization), name='Dense_1')(X_input)\n",
    "X_bn1 = BatchNormalization(name='BatchNorm_1')(X_dense1)\n",
    "X_dense2 = Dense(layer2_depth, activation=relu, kernel_regularizer=regularizers.l2(regularization), name='Dense_2')(X_bn1)\n",
    "X_bn2 = BatchNormalization(name='BatchNorm_2')(X_dense2)\n",
    "aux_input = Input(shape=(num_aux_input,), name='aux_input')\n",
    "X_concat = keras.layers.concatenate([X_bn2,aux_input],name='concat')\n",
    "X_bn3 = BatchNormalization(name='BatchNorm_3')(X_concat)\n",
    "X_dense3 = Dense(layer3_depth, activation=relu, kernel_regularizer=regularizers.l2(regularization), name='Dense_3')(X_bn3)\n",
    "X_bn4 = BatchNormalization(name='BatchNorm_4')(X_dense3)\n",
    "X_output = Dense(3, activation='softmax', name='Output')(X_bn4)\n",
    "model = Model(inputs=[X_input,aux_input],outputs=X_output)\n",
    "sgd = SGD(lr=0.1,decay=0.005,nesterov=False)\n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Create auxiliary input dictionary\n",
    "aux_dict = dict()\n",
    "aux_dict['X_train_aux_input'] = X_train_aux_input_8\n",
    "aux_dict['X_test_aux_input'] = X_test_aux_input_8\n",
    "\n",
    "# Create class weight dictionary\n",
    "class_weight_dict = dict()\n",
    "fracts = np.array([0.16,0.01,0.83])\n",
    "invFracts = np.power((1/fracts),alpha)\n",
    "weights = invFracts/(np.sum(invFracts))\n",
    "class_weight_dict = {0:weights[0],1:weights[1],2:weights[2]}\n",
    "\n",
    "# Save model to H5 file \n",
    "model_name = 'amp_model_demo.h5'\n",
    "model.save(model_name)\n",
    "# Run Stratified K-Fold Cross Validation\n",
    "all_dict = cv_trainingset(model_name,X_train.values,Y_train,X_test.values,Y_test.values,numsamples,cv_dict,aux_dict,class_weight_dict)\n",
    "\n",
    "print('Training Set Performance Metrics')\n",
    "print(\"FPR: %0.2f (+/- %0.3f)\" % (all_dict['fpr_train'][0]*100, all_dict['fpr_train'][1]*100 * 2))\n",
    "print(\"FNR: %0.2f (+/- %0.3f)\" % (all_dict['fnr_train'][0]*100, all_dict['fnr_train'][1]*100 * 2))\n",
    "print(\"TPR: %0.2f (+/- %0.3f)\" % (all_dict['tpr_train'][0]*100, all_dict['tpr_train'][1]*100 * 2))\n",
    "print(\"TNR: %0.2f (+/- %0.3f)\" % (all_dict['tnr_train'][0]*100, all_dict['tnr_train'][1]*100 * 2))\n",
    "\n",
    "print('Test Set Performance Metrics')\n",
    "print(\"FPR: %0.2f (+/- %0.3f)\" % (all_dict['fpr_test'][0]*100, all_dict['fpr_test'][1]*100 * 2))\n",
    "print(\"FNR: %0.2f (+/- %0.3f)\" % (all_dict['fnr_test'][0]*100, all_dict['fnr_test'][1]*100 * 2))\n",
    "print(\"TPR: %0.2f (+/- %0.3f)\" % (all_dict['tpr_test'][0]*100, all_dict['tpr_test'][1]*100 * 2))\n",
    "print(\"TNR: %0.2f (+/- %0.3f)\" % (all_dict['tnr_test'][0]*100, all_dict['tnr_test'][1]*100 * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**: Your expected output should be close to the following:\n",
    "\n",
    "<table> \n",
    "<tr>\n",
    "    <td> \n",
    "    **Training Set FPR =**\n",
    "    </td>\n",
    "\n",
    "    <td> \n",
    "      0.190\n",
    "    </td> \n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **Training Set FNR =**\n",
    "    </td>\n",
    "\n",
    "    <td> \n",
    "      0.048\n",
    "    </td> \n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **Test Set FPR =**\n",
    "    </td>\n",
    "\n",
    "    <td> \n",
    "      0.021\n",
    "    </td> \n",
    "</tr> \n",
    "\n",
    "<tr>\n",
    "    <td> \n",
    "    **Test Set FNR =**\n",
    "    </td>\n",
    "\n",
    "    <td> \n",
    "      0.085\n",
    "    </td> \n",
    "</tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Section - Visualization\n",
    "## 1.0 - Plotting Model Architecture\n",
    "\n",
    "Run the cell below to create a graphic visualization of the model architecture you created for this lesson. The visualization will be an image called 'model.png' saved in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon locating the \"model.png\" file and opening it, ithe image should look similar to the following (the details depend on what model you designed prior to this section):\n",
    "<img src=\"visualization_pictures/model.png\" style=\"width:700px;height:800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 - Tensorboard\n",
    "\n",
    "Tensorboard is an excellent visualization tool offered by Tensorflow, described by one of its developers as a \"flashlight to shine on the black box of neural networks.\" Tensorboard will allow you to not only see a graph of your model architecture, as we created in the previous section using Keras, but also graphs of accuracy over time, loss over time, histograms of weight values over time, and much more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Setting up\n",
    "\n",
    "To begin, we will be creating a new folder called \"tb_logs\" in the directory this notebook is saved on your system. As your neural network trains, information will be saved every epoch in the form of logs, which Tensorboard will use to create its visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to create the \"tb_logs\" folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/Users/christopher.chin/Documents/Project_Part_1/Demo_Files/tb_logs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1a51314761df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Specify the absolute path of the directory you will be saving the log files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mLOG_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolder_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/tb_logs'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOG_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/Users/christopher.chin/Documents/Project_Part_1/Demo_Files/tb_logs'"
     ]
    }
   ],
   "source": [
    "# Specify the absolute path of the directory you will be saving the log files \n",
    "LOG_DIR = folder_path+'/tb_logs'\n",
    "os.mkdir(LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, change (or leave as default) the parameters in the cell below, which will determine your neural network design. We will be working with a simpler 4-layer network in this section with no auxiliary inputs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run K.clear_session() to clear old models and layers from memory to speed up execution in this section\n",
    "# K.clear_session()\n",
    "\n",
    "# Set parameters\n",
    "layer1_depth = 40\n",
    "layer1_activation = 'relu'\n",
    "layer2_depth = 20\n",
    "layer2_activation = 'relu'\n",
    "layer3_depth = 20\n",
    "layer3_activation = 'relu'\n",
    "layer4_depth = 16\n",
    "layer4_activation = 'relu'\n",
    "epochs = 30\n",
    "batch_size = 200\n",
    "\n",
    "# Create Neural Network model\n",
    "model = Sequential()\n",
    "model.add(Dense(layer1_depth, activation=layer1_activation, name='dense_1', input_dim=40))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(layer2_depth, activation=layer2_activation, name='dense_2'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(layer3_depth, activation=layer3_activation, name='dense_3'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(layer4_depth, activation=layer4_activation, name='dense_4'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(3, activation='softmax', name='dense_5'))\n",
    "sgd = SGD(lr=0.1,decay=0.005,nesterov=False)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Starting Tensorboard\n",
    "\n",
    "As you will see going through this section, there are a lot of moving parts required to get Tensorboard up and running. In order to get you started, the following cell encapsulates a lot of those moving parts and we will not be going through all the details. Feel free, however, to read through the comments to see what is going on under the hood. \n",
    "\n",
    "For the purposes of this tutorial, there are a few points you should know about how Keras is using Tensorboard:\n",
    "- Tensorboard obtains information/performs calculations to assess the performance of the neural network every epoch, with two of the most important calculations it performs being accuracy and loss. It is important to note that Tensorboard is capable of performing these calculations not only on the training set (default), but also on an optionally provided validation/test set (with accompanying annotations). We will be using this feature of Tensorboard extensively to see the performance of the neural network on the training set and test set every epoch. \n",
    "- In addition to accuracy and loss, as we will see in the subsequent sections, Tensorboard also generates, for every epoch, histograms/distributions of the weight values, as well as \"embeddings\" (to be discussed) of the data for each layer. Because these specialized computations are time-consuming, Tensorboard has arguments to specify the frequency with which you want to perform them in units of epochs. We will be calling this \"calculations_freq\" and setting the default to 10, i.e. Tensorboard will perform the specialized calculations every 10 epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change (or leave as default) the value of \"calcluations_freq\" below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The frequency, in units of epochs, with which Tensorboard should perform specialized calculations\n",
    "calculations_freq = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to initialize Tensorboard. The details of the code are not important for this tutorial: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/christopher.chin/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Train on 51200 samples, validate on 12801 samples\n",
      "Epoch 1/30\n",
      "51200/51200 [==============================] - 2s 32us/step - loss: 0.1108 - acc: 0.9722 - val_loss: 0.0738 - val_acc: 0.9770\n",
      "Epoch 2/30\n",
      "51200/51200 [==============================] - 1s 18us/step - loss: 0.0691 - acc: 0.9789 - val_loss: 0.0623 - val_acc: 0.9803\n",
      "Epoch 3/30\n",
      "51200/51200 [==============================] - 1s 18us/step - loss: 0.0660 - acc: 0.9790 - val_loss: 0.0601 - val_acc: 0.9801\n",
      "Epoch 4/30\n",
      "51200/51200 [==============================] - 1s 18us/step - loss: 0.0637 - acc: 0.9795 - val_loss: 0.0594 - val_acc: 0.9803\n",
      "Epoch 5/30\n",
      "51200/51200 [==============================] - 1s 18us/step - loss: 0.0633 - acc: 0.9795 - val_loss: 0.0575 - val_acc: 0.9807\n",
      "Epoch 6/30\n",
      "51200/51200 [==============================] - 1s 18us/step - loss: 0.0618 - acc: 0.9798 - val_loss: 0.0590 - val_acc: 0.9804\n",
      "Epoch 7/30\n",
      "51200/51200 [==============================] - 1s 18us/step - loss: 0.0621 - acc: 0.9793 - val_loss: 0.0566 - val_acc: 0.9810\n",
      "Epoch 8/30\n",
      "51200/51200 [==============================] - 1s 18us/step - loss: 0.0607 - acc: 0.9800 - val_loss: 0.0564 - val_acc: 0.9802\n",
      "Epoch 9/30\n",
      "51200/51200 [==============================] - 1s 18us/step - loss: 0.0607 - acc: 0.9798 - val_loss: 0.0566 - val_acc: 0.9805\n",
      "Epoch 10/30\n",
      "51200/51200 [==============================] - 1s 18us/step - loss: 0.0599 - acc: 0.9799 - val_loss: 0.0558 - val_acc: 0.9804\n",
      "Epoch 11/30\n",
      "51200/51200 [==============================] - 1s 18us/step - loss: 0.0585 - acc: 0.9805 - val_loss: 0.0561 - val_acc: 0.9802\n",
      "Epoch 12/30\n",
      "51200/51200 [==============================] - 1s 19us/step - loss: 0.0593 - acc: 0.9804 - val_loss: 0.0563 - val_acc: 0.9804\n",
      "Epoch 13/30\n",
      "51200/51200 [==============================] - 1s 19us/step - loss: 0.0590 - acc: 0.9806 - val_loss: 0.0557 - val_acc: 0.9803\n",
      "Epoch 14/30\n",
      "51200/51200 [==============================] - 1s 19us/step - loss: 0.0588 - acc: 0.9802 - val_loss: 0.0551 - val_acc: 0.9805\n",
      "Epoch 15/30\n",
      "51200/51200 [==============================] - 1s 18us/step - loss: 0.0590 - acc: 0.9801 - val_loss: 0.0550 - val_acc: 0.9806\n",
      "Epoch 16/30\n",
      "51200/51200 [==============================] - 1s 19us/step - loss: 0.0579 - acc: 0.9802 - val_loss: 0.0549 - val_acc: 0.9804\n",
      "Epoch 17/30\n",
      "51200/51200 [==============================] - 1s 18us/step - loss: 0.0585 - acc: 0.9805 - val_loss: 0.0555 - val_acc: 0.9807\n",
      "Epoch 18/30\n",
      "51200/51200 [==============================] - 1s 19us/step - loss: 0.0574 - acc: 0.9807 - val_loss: 0.0552 - val_acc: 0.9806\n",
      "Epoch 19/30\n",
      "51200/51200 [==============================] - 1s 19us/step - loss: 0.0581 - acc: 0.9808 - val_loss: 0.0548 - val_acc: 0.9801\n",
      "Epoch 20/30\n",
      "51200/51200 [==============================] - 1s 19us/step - loss: 0.0578 - acc: 0.9802 - val_loss: 0.0550 - val_acc: 0.9804\n",
      "Epoch 21/30\n",
      "51200/51200 [==============================] - 1s 19us/step - loss: 0.0577 - acc: 0.9804 - val_loss: 0.0552 - val_acc: 0.9806\n",
      "Epoch 22/30\n",
      "51200/51200 [==============================] - 1s 19us/step - loss: 0.0579 - acc: 0.9802 - val_loss: 0.0548 - val_acc: 0.9803\n",
      "Epoch 23/30\n",
      "51200/51200 [==============================] - 1s 19us/step - loss: 0.0580 - acc: 0.9801 - val_loss: 0.0551 - val_acc: 0.9805\n",
      "Epoch 24/30\n",
      "51200/51200 [==============================] - 1s 18us/step - loss: 0.0570 - acc: 0.9803 - val_loss: 0.0547 - val_acc: 0.9805\n",
      "Epoch 25/30\n",
      "51200/51200 [==============================] - 1s 19us/step - loss: 0.0573 - acc: 0.9808 - val_loss: 0.0548 - val_acc: 0.9807\n",
      "Epoch 26/30\n",
      "51200/51200 [==============================] - 1s 20us/step - loss: 0.0571 - acc: 0.9803 - val_loss: 0.0551 - val_acc: 0.9800\n",
      "Epoch 27/30\n",
      "51200/51200 [==============================] - 1s 19us/step - loss: 0.0566 - acc: 0.9807 - val_loss: 0.0547 - val_acc: 0.9802\n",
      "Epoch 28/30\n",
      "51200/51200 [==============================] - 1s 19us/step - loss: 0.0577 - acc: 0.9807 - val_loss: 0.0547 - val_acc: 0.9802\n",
      "Epoch 29/30\n",
      "51200/51200 [==============================] - 1s 19us/step - loss: 0.0578 - acc: 0.9802 - val_loss: 0.0549 - val_acc: 0.9802\n",
      "Epoch 30/30\n",
      "51200/51200 [==============================] - 1s 19us/step - loss: 0.0567 - acc: 0.9806 - val_loss: 0.0548 - val_acc: 0.9806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a28b38cf8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a one-hot encoding of Y_test \n",
    "Y_test_onehot = to_categorical(Y_test)\n",
    "# Specify which layers Tensorboard should derive embeddings from\n",
    "embedding_layer_names = set(layer.name for layer in model.layers if layer.name.startswith('dense_'))\n",
    "# Specify what data Tensorboard should use to calculate the embeddings \n",
    "embeddings_data = X_test.values\n",
    "\n",
    "# Create metadata file containing annotations associated with each example that will be embedded \n",
    "metadata_file = os.path.join(LOG_DIR, 'metadata.tsv')\n",
    "with open(metadata_file, 'w') as f:\n",
    "    for ii in range(len(Y_test.values)):\n",
    "        f.write('{}\\n'.format(int(Y_test.values[ii])-1))\n",
    "\n",
    "# Define the Tensorboard callback. \n",
    "tensorboard = TensorBoard(log_dir=LOG_DIR, histogram_freq=calculations_freq, batch_size=32,\n",
    "                           write_graph=True, write_grads=False, write_images=True,\n",
    "                           embeddings_freq=calculations_freq, embeddings_metadata=metadata_file,\n",
    "                           embeddings_layer_names=embedding_layer_names,\n",
    "                           embeddings_data=embeddings_data)\n",
    "# Fit the model \n",
    "model.fit(X_train,Y_train,callbacks=[tensorboard],epochs=epochs,batch_size=batch_size,validation_data=(X_test,Y_test_onehot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the cell above is running, open Terminal (for Mac) or Command Prompt (for Windows). For Mac Users in Terminal, you will see something like the following: \n",
    "\n",
    "<img src=\"visualization_pictures/terminal_start_screen.png\" style=\"width:800px;height:200px;\">\n",
    "\n",
    "Note the absolute path on your system where the \"tb_logs\" folder is stored. Enter the following command in Terminal/Command Prompt, replacing \"[path]\" with your absolute path:\n",
    "\n",
    "**\"tensorboard --logdir [path]\"**\n",
    "\n",
    "In my case, \"[path]\" was \"/Users/christopher.chin/tb_logs.\" After entering the command in Terminal/Command Prompt your window should be similar to the following: \n",
    "\n",
    "<img src=\"visualization_pictures/terminal_command.png\" style=\"width:800px;height:200px;\">\n",
    "\n",
    "Press \"Enter\" and the command will run, starting up Tensorboard. \n",
    "\n",
    "In order to access Tensorboard, you will have to open a new window in the Web Browser of your choice and enter the following in the address bar. Note that \"6006\" is the port on your localhost assigned to Tensorboard: \n",
    "\n",
    "**\"localhost:6006\"**\n",
    "\n",
    "After entering the line above in your browser's address bar, your screen should be similar to the following: \n",
    "\n",
    "<img src=\"visualization_pictures/browser_start_screen.png\" style=\"width:475px;height:100px;\">\n",
    "\n",
    "Press \"Enter\" and you will be directed to the server running Tensorboard. You should see a screen in your browser similar to the following. (If your start-up screen is initially very different, the \"Scalars\" dashboard in the top bar of the page may not yet be available. In that case, wait a little for Tensorboard to perform its computations. Within a few minutes, the \"Scalars\" dashboard option should appear and clicking on it will produce the start-up screen below):   \n",
    "\n",
    "<img src=\"visualization_pictures/tb_start_screen.png\" style=\"width:1000px;height:500px;\">\n",
    "\n",
    "On the top bar of the screen are the different dashboards you have access to:\n",
    "- **Scalars** \n",
    "\n",
    "- **Images** \n",
    "\n",
    "- **Graphs**\n",
    "\n",
    "- **Distributions**\n",
    "\n",
    "- **Histograms** \n",
    "\n",
    "- **Projector** \n",
    "\n",
    "We will be going through each of these dashboards step-by-step in the next few sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Scalars Dashboard\n",
    "We will be focusing in this section on the \"Scalars\" dashboard. Examine the screenshot below:\n",
    "\n",
    "<img src=\"visualization_pictures/tb_start_screen_edited.png\" style=\"width:1000px;height:500px;\">\n",
    "\n",
    "The Scalars dashboard gives you visualizations of all the scalar information Tensorboard has collected over the epochs of training, specifically the accuracy and loss. As noted previously, every epoch the model will evaluate the accuracy and loss on the training set (indicated by the top two white tabs \"acc\" and \"loss\"), but also the accuracy and loss on the test set (indicated by the bottom two white tabs \"val_acc\" and \"val_loss\"). You can click on the white tabs themselves to show or hide the corresponding graphs. Note that \"val_loss\" does not appear in the screenshot above, but is beneath the white \"val_acc\" tab if you continue to scroll down on the page. \n",
    "\n",
    "All of the graphs can be manipulated in the same way, so we will examine the training accuracy \"acc\" plot as a representative model. As indicated in the screenshot, click the left-most button to enlarge the image. You should see something similar to the following: \n",
    "\n",
    "<img src=\"visualization_pictures/acc_edited.png\" style=\"width:1000px;height:500px;\">\n",
    "\n",
    "As indicated in the screenshot, there is a section on the left-hand-side pane called \"Smoothing\" that contains a sliding bar and box that accepts numerical inputs. By default, the graph of accuracy is smoothed, with degree of smoothing 0.6, to more easily view the trend in the graph. You can reduce the degree of smoothing to 0, or increase the smoothing to 1 by sliding the bar or entering the desired value into the box. By reducing the smoothing to 0, you can see a graph similar to the following:\n",
    "\n",
    "<img src=\"visualization_pictures/acc_smoothed.png\" style=\"width:1000px;height:500px;\">\n",
    "\n",
    "Note also that moving your cursor along the graph provides you with a black box containing information about the specific training accuracy values calculated every epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Images Dashboard\n",
    "\n",
    "The Images dashboard allows you to visualize the weights that the neural network is learning over time. Clicking on the \"Images\" dashboard at the top of the Tensorboard page should open a windows similar to the following: \n",
    "\n",
    "<img src=\"visualization_pictures/images_start_screen.png\" style=\"width:1000px;height:500px;\">\n",
    "\n",
    "As indicated in the screenshot, click the white tab labeled \"batch_normalization_1\" to close it. Close all other open tabs until the screen looks like the following: \n",
    "\n",
    "<img src=\"visualization_pictures/images_mid_screen.png\" style=\"width:1000px;height:500px;\">\n",
    "\n",
    "As indicated in the screenshot, click the white tab labeled \"dense_1\" to open it. Your screen should now look similar to the following:\n",
    "\n",
    "<img src=\"visualization_pictures/images_final_screen.png\" style=\"width:1000px;height:500px;\">\n",
    "\n",
    "We are now looking at a visualization of the bias vector and weight matrix for the first hidden layer (called \"dense_1\") of the neural network. Darker colors correspond to values of larger magnitude while lighter colors correspond to weights of smaller magnitude. In order to interpret these images, recall that neural network computations follow the series of steps described below. This entire set of computations occurs for every neuron in every layer: \n",
    "\n",
    "<img src=\"visualization_pictures/nn_computation.png\" style=\"width:600px;height:300px;\">\n",
    "\n",
    "Let's fix one neuron in a given layer as an example. The first part of the calculations is the \"pre-activation,\" which is computed by taking the sum of the products of the outputs of all neurons in the previous layer with their corresponding weighted connections to the fixed neuron under consideration. A bias vector is then added to the result. In the second part of the calculations, an \"activation function\" is applied to the \"pre-activation\" output to yield the final output. Therefore, since the first hidden layer \"dense_1\" in the network contains 40 neurons and the previous layer - the input layer - is 40-dimensional, there must be 40 weighted connections per neuron in \"dense_1.\" Since the \"pre-activation\" computation can be summarized as a matrix-vector multiplication (between a weight matrix and the output vector of the previous layer), the weight matrix associated with \"dense_1\" must be of shape (40x40). (Multiplying the 40x40 weight matrix by the 40x1 vector of inputs yields a 40x1 vector representing the \"pre-activation\" output). The  bias vector must then be of shape (40x1), as it is added to the result of the matrix-vector multiplication. \n",
    "The weight matrix is reproduced and enlarged below:\n",
    "\n",
    "<img src=\"visualization_pictures/weight_matrix.png\" style=\"width:550px;height:400px;\">\n",
    "\n",
    "The weight matrix can be interpreted as follows:\n",
    "- Every row (i) in the matrix corresponds to the weighted connections between neuron (i) in the given layer and all the inputs from the previous layer. \n",
    "- Every element (j) in a row corresponds to the weighted connection between neuron (i) in the given layer and input (j) from the previous layer. \n",
    "- *Example: Consider the topmost row of the weight matrix, which represents the weighted connections between the first neuron in \"dense_1\" and all the elements of the input vector. The first element in the row represents the connection between the first neuron in \"dense_1\" and the first element of the input vector, which corresponds to the fluorescence value at cycle 1. Likewise, the last element in the row represents the connection between the first neuron in \"dense_1\" and the last element of the input vector, which corresponds to the fluorescence value at cycle 40.*\n",
    "\n",
    "As indicated in the screenshot, you can also move the \"Epochs slider\" from right to left to see the weight matrix values at earlier epochs. The same operations can also be done on the bias vector visualization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Graphs Dashboard\n",
    "\n",
    "In Bonus Section 1.0, we plotted your model architecture using Keras as a high-level flow-diagram showing the layer names, layer sizes, and connections between layers. The Graphs Tab of Tensorboard functions as a more in-depth, low-level visualization of your model. After selecting the \"Graphs\" dashboard, your screen should be similar to the following:\n",
    "\n",
    "<img src=\"visualization_pictures/graphs_start_screen.png\" style=\"width:1000px;height:500px;\">\n",
    "\n",
    "There are a lot of details on this page that I encourage you to expore on your own. For the purposes of this tutorial, we will focus on the flow-diagram on the left titled \"Main Graph,\" which shows all the connections between layers and serves as a similar graphical rendition to the Keras visualization we plotted earlier. Here, however, you can interact with the graph and double-click on a layer to show the inner computations. \n",
    "As indicated in the screenshot, double-click on \"dense_4\" to show more details about the layer:\n",
    "\n",
    "<img src=\"visualization_pictures/graphs_final_screen.png\" style=\"width:1000px;height:500px;\">\n",
    "\n",
    "Upon opening up the \"dense_4\" layer, you can now see the computations with the weight matrix, bias vector, the regularization applied, etc. Opening up all the layers in this fashion would allow you to follow the backpropagation of the gradient through all the layers of the network (used to update all weight parameters as the model trains). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - Distributions Dashboard\n",
    "\n",
    "The Distributions dashboard is a variation upon the Images dashboard, in the sense that it allows you to see the evolution of the weight matrices and bias vectors in your model over time. This time, however, the visualization takes the form of distribution, as the heading suggests. Upon clicking on the \"Distributions\" dashboard, your screen should be similar to the following: \n",
    "\n",
    "<img src=\"visualization_pictures/distributions_start_screen.png\" style=\"width:1000px;height:500px;\">\n",
    "\n",
    "As indicated in the screenshot, click on the white tab titled \"batch_normalization_1\" to close it. Close any other open white tabs in a similar manner until your screen resembles the following:\n",
    "\n",
    "<img src=\"visualization_pictures/distributions_mid_screen.png\" style=\"width:1000px;height:500px;\">\n",
    "\n",
    "As indicated in the screenshot, click on the \"dense_1\" tab to open it. You should now see graphs similar to the ones below: \n",
    "\n",
    "<img src=\"visualization_pictures/distributions_final_screen.png\" style=\"width:1000px;height:500px;\">\n",
    "\n",
    "Here is how to interpret the graphs:\n",
    "- **Horizontal Axis:** Epochs\n",
    "\n",
    "- **Vertical Axis:** Values of the weights/bias\n",
    "\n",
    "- **Horizontal Lines:** The horizontal lines cutting through the graphs represent percentiles of the distribution over the weights/biases. From top to bottom, the lines represent the maximum, 93%, 84%, 69%, 50%, 31%, 16%, 7%, and minimum percentiles in the distribution. Following an individual line across the graph shows how that particular percentile has over the epochs of training. For instance, follow the highest line in the graphs to see how the maximum value changes over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 - Histograms Dashboard\n",
    "\n",
    "The Histograms dashboard contains rotated views of the graphs from the Distributions dashboard, so we will not spend too much time in this section. However, the different perspective can provide different insights into how the weights/biases are changing over time. Upon clicking on the Histograms dashboard, closing all open white tabs, and opening the white tab titled \"dense_1,\" your screen should be similar to the following:\n",
    "\n",
    "<img src=\"visualization_pictures/histograms_final_screen.png\" style=\"width:1000px;height:500px;\">\n",
    "\n",
    "Once again, these graphs are the same as those we saw in the previous section on the Distributions tab, only rotated so that the following interpretation holds:\n",
    "- **Horizontal Axis:** Values of the weights/bias\n",
    "\n",
    "- **Vertical Axis:** Epochs\n",
    "\n",
    "- **Horizontal Slices:** The horizontal slices that appear on individual horizontal axes in the graphs represent histograms of the weight/bias values at a particular epoch. Histograms for earlier, older epochs appear in the \"back,\" while histograms for later, more recent epochs appear in the front. Looking at an individual temporal slice histogram in this way allows you to see the distribution of the weights/bias values at a given moment in time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 - Projector Dashboard\n",
    "\n",
    "One of the most exciting features of Tensorboard is the visualization of embeddings. Typically, embeddings refer to the conversion of qualitiative objects like words into numerical vectors, which can then be compared on the basis of similarity metrics like distance. More generally, however, embeddings can be thought of as function mappings from input to output. Each layer of a neural network can therefore be thought of as creating an embedding of the input feature vector, transforming it into a new-dimensional space. For example, given the 40-dimensional input feature vector we have been considering thus far (corresponding to the 40 fluorescence values for each cycle), if the first hidden layer has 40 neurons, it is embedding or transforming the input feature vector into a new 40-dimensional space. The second hidden layer, which has 20 neurons, is then transforming its input into a new 20-dimensional space, and so on. The Projector dashboard allows us to visualize these high-dimensional embeddings by translating them into 2 or 3 dimensions via algorithms like PCA (taking the first two or three principal components), or t-SNE. \n",
    "\n",
    "Upon clicking on the Projector Dashboard, your screen should be similar to the following. (If you instead receive an error message saying \"Error fetching tensors\" the projector dashboard is not yet ready for viewing, as Tensorboard may still be performing its calculations. Wait a few minutes and re-visit the dashboard. Typically, the Projector dashboard is fully accessible once the neural network has finished training completely):\n",
    "\n",
    "<img src=\"visualization_pictures/projector_start_screen.png\" style=\"width:1000px;height:500px;\">\n",
    "\n",
    "Each data point in this space represents an example in our training set. The visualization right now does not have much meaning, however, because we are unable to identify which data point belongs to which class. We will be distignuishing the different classes by color. First, as indicated in the screenshot, click on the dropdown menu titled \"Color by\": \n",
    "\n",
    "<img src=\"visualization_pictures/projector_label_menu.png\" style=\"width:1000px;height:500px;\">\n",
    "\n",
    "Now select the option \"label\": \n",
    "\n",
    "<img src=\"visualization_pictures/projector_colored.png\" style=\"width:1000px;height:500px;\">\n",
    "\n",
    "All the data points are now colored according to the class they correspond to. (Important: Note that the colors on your screen may differ from those depicted in the screenshots. Hover your cursor over individual, colored data points to see which color corresponds to which class). In the screenshots, red corresponds to \"Amplified (Label: 1)\", orange corresponds to \"Inconclusive (Label: 0)\", and blue corresponds to \"Non-Amplified (Label: -1).\" Representative points belonging to each of these classes are labeled in the screenshot. Feel free to explore the visualization by clicking individual data points, clicking and dragging to rotate the plot, etc. You can see that the first hidden layer of the neural network has found an embedding for the different classes, in which each class belongs to a to different region of a high-dimensional manifold. \n",
    "Now click on the dropdown menu on the top of the left-hand-side pane titled \"5 tensors found.\" Select \"dense_5_embedding\" and your screen should be similar to the following:\n",
    "\n",
    "<img src=\"visualization_pictures/projector_final_screen.png\" style=\"width:1000px;height:500px;\">\n",
    "\n",
    "This visualization represents the embedding of the last layer of the neural network. Once again, the neural network has found an embedding where each class belongs to a different region in the high-dimensional space. Data points that are \"close\" to each other in the space (\"closeness\" being defined by a distance metric) are more similar to each other and more likely to belong to the same class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!!!\n",
    "\n",
    "That concludes our tutorial. Hopefully, this demo provided a good starting point for you to begin working with your own neural network designs. As a final note, the Keras documentation is an excellent resource for future inquiries and additional exploration:\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "*In addition, note that if you wish to use Tensorboard after finishing this demo, you must delete all the contents of your tb_logs folder so that Tensorboard does not overlap your new graphs with graphs of previous models.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
